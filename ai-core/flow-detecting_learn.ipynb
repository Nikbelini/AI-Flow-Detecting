{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6663b040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd720198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка предобученной модели YOLOv10 (можно 'yolov10n.pt', 'yolov10s.pt' или yolov10m.pt)\n",
    "model = YOLO('yolov10n.pt')  # nano-версия (быстрая, но менее точная)\n",
    "\n",
    "model = YOLO('yolov10m.pt')  # рекомендуемая модель к обучению\n",
    "\n",
    "# Обучение модели\n",
    "results = model.train(data='dataset/data.yaml', epochs=100, imgsz=640, model='yolov8m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a145f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "results = model.train(data='dataset/data.yaml', epochs=100, imgsz=640, model='yolov8m.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4053843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8s-seg.pt')  # или yolov8n-seg.pt для быстрого старта\n",
    "model.train(data='data.yaml', epochs=100, imgsz=640, batch=16,\n",
    "            augment=True, lr0=0.01, patience=50, project='runs/exp_passengers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0600e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 000000.jpg\n",
      "Saved 000001.jpg\n",
      "Saved 000002.jpg\n",
      "Saved 000003.jpg\n",
      "Saved 000004.jpg\n",
      "Saved 000005.jpg\n",
      "Saved 000006.jpg\n",
      "Saved 000007.jpg\n",
      "Saved 000008.jpg\n",
      "Saved 000009.jpg\n",
      "Saved 000010.jpg\n",
      "Saved 000011.jpg\n",
      "Saved 000012.jpg\n",
      "Saved 000013.jpg\n",
      "Saved 000014.jpg\n",
      "Saved 000015.jpg\n",
      "Saved 000016.jpg\n",
      "Saved 000017.jpg\n",
      "Saved 000018.jpg\n",
      "Saved 000019.jpg\n",
      "Saved 000020.jpg\n",
      "Saved 000021.jpg\n",
      "Saved 000022.jpg\n",
      "Saved 000023.jpg\n",
      "Saved 000024.jpg\n",
      "Saved 000025.jpg\n",
      "Saved 000026.jpg\n",
      "Saved 000027.jpg\n",
      "Saved 000028.jpg\n",
      "Saved 000029.jpg\n",
      "Saved 000030.jpg\n",
      "Saved 000031.jpg\n",
      "Saved 000032.jpg\n",
      "Saved 000033.jpg\n",
      "Saved 000034.jpg\n",
      "Saved 000035.jpg\n",
      "Saved 000036.jpg\n",
      "Saved 000037.jpg\n",
      "Saved 000038.jpg\n",
      "Saved 000039.jpg\n",
      "Saved 000040.jpg\n",
      "Saved 000041.jpg\n",
      "Saved 000042.jpg\n",
      "Saved 000043.jpg\n",
      "Saved 000044.jpg\n",
      "Saved 000045.jpg\n",
      "Saved 000046.jpg\n",
      "Saved 000047.jpg\n",
      "Saved 000048.jpg\n",
      "Saved 000049.jpg\n",
      "Saved 000050.jpg\n",
      "Saved 000051.jpg\n",
      "Saved 000052.jpg\n",
      "Saved 000053.jpg\n",
      "Saved 000054.jpg\n",
      "Saved 000055.jpg\n",
      "Saved 000056.jpg\n",
      "Saved 000057.jpg\n",
      "Saved 000058.jpg\n",
      "Saved 000059.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m saved = \u001b[32m0\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     ret, frame = \u001b[43mcap\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     19\u001b[39m         time.sleep(\u001b[32m0.5\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# capture_frames.py\n",
    "\n",
    "\n",
    "RTSP_URL = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"  # подставь свой поток\n",
    "OUT_DIR = \"dataset/images\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(RTSP_URL)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
    "grab_every = int(fps)  # один кадр в секунду (при fps ~1 кадр/sec, уменьшить)\n",
    "\n",
    "i = 0\n",
    "saved = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        time.sleep(0.5)\n",
    "        continue\n",
    "    if i % grab_every == 0:\n",
    "        fname = f\"{saved:06d}.jpg\"\n",
    "        cv2.imwrite(os.path.join(OUT_DIR, fname), frame)\n",
    "        saved += 1\n",
    "        print(\"Saved\", fname)\n",
    "    i += 1\n",
    "    # остановка вручную:\n",
    "    if saved >= 500:  # например 500 кадров\n",
    "        break\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c67868c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov10n.pt to 'yolov10n.pt': 100%|██████████| 5.59M/5.59M [00:08<00:00, 669kB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000000.jpg: 384x640 1 person, 7 cars, 51.1ms\n",
      "Speed: 1.8ms preprocess, 51.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000001.jpg: 384x640 1 person, 5 cars, 44.7ms\n",
      "Speed: 2.9ms preprocess, 44.7ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000002.jpg: 384x640 3 persons, 7 cars, 32.6ms\n",
      "Speed: 2.0ms preprocess, 32.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000003.jpg: 384x640 5 persons, 7 cars, 38.2ms\n",
      "Speed: 2.0ms preprocess, 38.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000004.jpg: 384x640 1 person, 7 cars, 41.2ms\n",
      "Speed: 1.4ms preprocess, 41.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000005.jpg: 384x640 1 person, 7 cars, 32.1ms\n",
      "Speed: 1.4ms preprocess, 32.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000006.jpg: 384x640 1 person, 5 cars, 34.0ms\n",
      "Speed: 1.6ms preprocess, 34.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000007.jpg: 384x640 1 person, 5 cars, 33.9ms\n",
      "Speed: 1.2ms preprocess, 33.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000008.jpg: 384x640 2 persons, 6 cars, 30.3ms\n",
      "Speed: 1.7ms preprocess, 30.3ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000009.jpg: 384x640 1 person, 4 cars, 33.9ms\n",
      "Speed: 1.5ms preprocess, 33.9ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000010.jpg: 384x640 4 persons, 6 cars, 32.9ms\n",
      "Speed: 1.8ms preprocess, 32.9ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000011.jpg: 384x640 3 persons, 5 cars, 29.9ms\n",
      "Speed: 2.0ms preprocess, 29.9ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000012.jpg: 384x640 3 persons, 5 cars, 48.1ms\n",
      "Speed: 1.3ms preprocess, 48.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000013.jpg: 384x640 3 persons, 5 cars, 27.9ms\n",
      "Speed: 1.4ms preprocess, 27.9ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000014.jpg: 384x640 4 persons, 5 cars, 26.3ms\n",
      "Speed: 1.3ms preprocess, 26.3ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000015.jpg: 384x640 5 persons, 5 cars, 30.9ms\n",
      "Speed: 1.6ms preprocess, 30.9ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000016.jpg: 384x640 4 persons, 6 cars, 34.0ms\n",
      "Speed: 2.1ms preprocess, 34.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000017.jpg: 384x640 3 persons, 6 cars, 32.7ms\n",
      "Speed: 1.7ms preprocess, 32.7ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000018.jpg: 384x640 3 persons, 5 cars, 29.8ms\n",
      "Speed: 1.3ms preprocess, 29.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000019.jpg: 384x640 3 persons, 5 cars, 28.2ms\n",
      "Speed: 1.5ms preprocess, 28.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000020.jpg: 384x640 3 persons, 6 cars, 31.1ms\n",
      "Speed: 1.4ms preprocess, 31.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000021.jpg: 384x640 1 person, 5 cars, 30.0ms\n",
      "Speed: 1.7ms preprocess, 30.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000022.jpg: 384x640 4 persons, 5 cars, 31.7ms\n",
      "Speed: 1.3ms preprocess, 31.7ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000023.jpg: 384x640 1 person, 6 cars, 29.8ms\n",
      "Speed: 1.5ms preprocess, 29.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000024.jpg: 384x640 4 persons, 6 cars, 33.0ms\n",
      "Speed: 1.8ms preprocess, 33.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000025.jpg: 384x640 3 persons, 5 cars, 32.6ms\n",
      "Speed: 1.2ms preprocess, 32.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000026.jpg: 384x640 2 persons, 5 cars, 30.2ms\n",
      "Speed: 1.3ms preprocess, 30.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000027.jpg: 384x640 4 persons, 7 cars, 31.5ms\n",
      "Speed: 1.7ms preprocess, 31.5ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000028.jpg: 384x640 2 persons, 4 cars, 31.9ms\n",
      "Speed: 1.3ms preprocess, 31.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000029.jpg: 384x640 4 persons, 5 cars, 31.8ms\n",
      "Speed: 1.9ms preprocess, 31.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000030.jpg: 384x640 5 persons, 5 cars, 32.4ms\n",
      "Speed: 1.3ms preprocess, 32.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000031.jpg: 384x640 3 persons, 4 cars, 28.8ms\n",
      "Speed: 1.3ms preprocess, 28.8ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000032.jpg: 384x640 3 persons, 4 cars, 29.8ms\n",
      "Speed: 1.5ms preprocess, 29.8ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000033.jpg: 384x640 3 persons, 4 cars, 31.4ms\n",
      "Speed: 1.3ms preprocess, 31.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000034.jpg: 384x640 3 persons, 3 cars, 29.4ms\n",
      "Speed: 1.2ms preprocess, 29.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000035.jpg: 384x640 5 persons, 3 cars, 31.0ms\n",
      "Speed: 1.3ms preprocess, 31.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000036.jpg: 384x640 4 persons, 4 cars, 31.5ms\n",
      "Speed: 1.3ms preprocess, 31.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000037.jpg: 384x640 4 persons, 4 cars, 30.1ms\n",
      "Speed: 1.5ms preprocess, 30.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000038.jpg: 384x640 4 persons, 4 cars, 29.0ms\n",
      "Speed: 1.3ms preprocess, 29.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000039.jpg: 384x640 4 persons, 4 cars, 30.2ms\n",
      "Speed: 1.3ms preprocess, 30.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000040.jpg: 384x640 5 persons, 4 cars, 32.4ms\n",
      "Speed: 1.5ms preprocess, 32.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000041.jpg: 384x640 2 persons, 4 cars, 32.8ms\n",
      "Speed: 1.5ms preprocess, 32.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000042.jpg: 384x640 3 persons, 4 cars, 30.1ms\n",
      "Speed: 1.4ms preprocess, 30.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000043.jpg: 384x640 5 persons, 4 cars, 31.3ms\n",
      "Speed: 1.4ms preprocess, 31.3ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000044.jpg: 384x640 4 persons, 4 cars, 29.9ms\n",
      "Speed: 1.5ms preprocess, 29.9ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000045.jpg: 384x640 3 persons, 5 cars, 32.6ms\n",
      "Speed: 1.7ms preprocess, 32.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000046.jpg: 384x640 3 persons, 5 cars, 30.6ms\n",
      "Speed: 1.5ms preprocess, 30.6ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000047.jpg: 384x640 4 persons, 6 cars, 35.0ms\n",
      "Speed: 2.1ms preprocess, 35.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000048.jpg: 384x640 1 person, 7 cars, 32.0ms\n",
      "Speed: 1.9ms preprocess, 32.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000049.jpg: 384x640 3 persons, 6 cars, 44.8ms\n",
      "Speed: 1.7ms preprocess, 44.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000050.jpg: 384x640 3 persons, 6 cars, 33.3ms\n",
      "Speed: 1.7ms preprocess, 33.3ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000051.jpg: 384x640 4 persons, 7 cars, 34.5ms\n",
      "Speed: 1.8ms preprocess, 34.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000052.jpg: 384x640 5 persons, 6 cars, 32.8ms\n",
      "Speed: 1.7ms preprocess, 32.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000053.jpg: 384x640 4 persons, 5 cars, 34.0ms\n",
      "Speed: 1.3ms preprocess, 34.0ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000054.jpg: 384x640 5 persons, 5 cars, 1 bus, 31.4ms\n",
      "Speed: 2.1ms preprocess, 31.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000055.jpg: 384x640 2 persons, 4 cars, 33.8ms\n",
      "Speed: 1.8ms preprocess, 33.8ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000056.jpg: 384x640 4 persons, 4 cars, 33.0ms\n",
      "Speed: 1.8ms preprocess, 33.0ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000057.jpg: 384x640 4 persons, 5 cars, 69.3ms\n",
      "Speed: 8.6ms preprocess, 69.3ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000058.jpg: 384x640 2 persons, 5 cars, 45.0ms\n",
      "Speed: 2.7ms preprocess, 45.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 e:\\AIM\\AI-Flow-Detecting\\ai-core\\dataset\\images\\000059.jpg: 384x640 2 persons, 3 cars, 31.5ms\n",
      "Speed: 1.6ms preprocess, 31.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os, json\n",
    "\n",
    "model = YOLO('yolov10n.pt')  # предобученная\n",
    "img_dir = \"dataset/images\"\n",
    "out_ann_dir = \"dataset/labels\" \n",
    "os.makedirs(out_ann_dir, exist_ok=True)\n",
    "\n",
    "for img in os.listdir(img_dir):\n",
    "    if not img.endswith('.jpg'): continue\n",
    "    res = model.predict(os.path.join(img_dir,img), imgsz=640, conf=0.3)\n",
    "    # res[0].boxes.xyxyn, res[0].boxes.cls, res[0].masks ... можно парсить и сохранять YOLO txt\n",
    "    boxes = res[0].boxes\n",
    "    h, w = res[0].orig_shape\n",
    "    lines = []\n",
    "    for b in boxes:\n",
    "        x1,y1,x2,y2 = b.xyxy[0].tolist()\n",
    "        cls = int(b.cls[0].item())\n",
    "        # convert to yolo format\n",
    "        cx = ((x1 + x2) / 2) / w\n",
    "        cy = ((y1 + y2) / 2) / h\n",
    "        bw = (x2 - x1) / w\n",
    "        bh = (y2 - y1) / h\n",
    "        lines.append(f\"{cls} {cx:.6f} {cy:.6f} {bw:.6f} {bh:.6f}\")\n",
    "    with open(os.path.join(out_ann_dir, img.replace('.jpg','.txt')), 'w') as f:\n",
    "        f.write(\"\\n\".join(lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bd55ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.176  Python-3.12.6 torch-2.8.0+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s-seg.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train5, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/exp_passengers, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\exp_passengers\\train5, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2770931  ultralytics.nn.modules.head.Segment          [1, 32, 128, [128, 256, 512]] \n",
      "YOLOv8s-seg summary: 151 layers, 11,790,483 parameters, 11,790,467 gradients, 42.7 GFLOPs\n",
      "\n",
      "Transferred 411/417 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1786.0367.8 MB/s, size: 963.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning E:\\AIM\\AI-Flow-Detecting\\ai-core\\data\\labels.cache... 0 images, 9 backgrounds, 0 corrupt: 100%|██████████| 9/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Labels are missing or empty in E:\\AIM\\AI-Flow-Detecting\\ai-core\\data\\labels.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 4522.81583.5 MB/s, size: 960.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning E:\\AIM\\AI-Flow-Detecting\\ai-core\\data\\labels.cache... 0 images, 9 backgrounds, 0 corrupt: 100%|██████████| 9/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING Labels are missing or empty in E:\\AIM\\AI-Flow-Detecting\\ai-core\\data\\labels.cache, training may not work correctly. See https://docs.ultralytics.com/datasets for dataset formatting guidance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs\\exp_passengers\\train5\\labels.jpg... \n",
      "WARNING zero-size array to reduction operation maximum which has no identity\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\exp_passengers\\train5\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      1/100         0G          0          0      400.1          0          0        640: 100%|██████████| 1/1 [00:07<00:00,  7.44s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          9          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:850: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\numpy\\_core\\_methods.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      2/100         0G          0          0      462.2          0          0        640: 100%|██████████| 1/1 [00:06<00:00,  6.42s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          9          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:850: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\numpy\\_core\\_methods.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      3/100         0G          0          0      272.6          0          0        640: 100%|██████████| 1/1 [00:05<00:00,  5.97s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          9          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:850: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\numpy\\_core\\_methods.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      4/100         0G          0          0      193.4          0          0        640: 100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          9          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:850: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\numpy\\_core\\_methods.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      5/100         0G          0          0      164.9          0          0        640: 100%|██████████| 1/1 [00:06<00:00,  6.23s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          9          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:850: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\numpy\\_core\\_methods.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      6/100         0G          0          0      146.7          0          0        640: 100%|██████████| 1/1 [00:05<00:00,  5.99s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          9          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:850: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\numpy\\_core\\_methods.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      7/100         0G          0          0      130.1          0          0        640: 100%|██████████| 1/1 [00:06<00:00,  6.22s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          9          0          0          0          0          0          0          0          0          0\n",
      "WARNING no labels found in segment set, can not compute metrics without labels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\metrics.py:850: RuntimeWarning: Mean of empty slice.\n",
      "  i = smooth(f1_curve.mean(0), 0.1).argmax()  # max F1 index\n",
      "e:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\numpy\\_core\\_methods.py:139: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = um.true_divide(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[32m      3\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolov8s-seg.pt\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# или yolov8n-seg.pt для быстрого старта\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m            \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mruns/exp_passengers\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\model.py:799\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    796\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m    798\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.hub_session = \u001b[38;5;28mself\u001b[39m.session  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m799\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:227\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    224\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:406\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self, world_size)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;28mself\u001b[39m.amp):\n\u001b[32m    405\u001b[39m     batch = \u001b[38;5;28mself\u001b[39m.preprocess_batch(batch)\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m     loss, \u001b[38;5;28mself\u001b[39m.loss_items = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    407\u001b[39m     \u001b[38;5;28mself\u001b[39m.loss = loss.sum()\n\u001b[32m    408\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m RANK != -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:138\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[33;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[32m    126\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predict(x, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:337\u001b[39m, in \u001b[36mBaseModel.loss\u001b[39m\u001b[34m(self, batch, preds)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcriterion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    335\u001b[39m     \u001b[38;5;28mself\u001b[39m.criterion = \u001b[38;5;28mself\u001b[39m.init_criterion()\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.criterion(preds, batch)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:139\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:157\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:180\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    181\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\modules\\head.py:271\u001b[39m, in \u001b[36mSegment.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: List[torch.Tensor]) -> Union[Tuple, List[torch.Tensor]]:\n\u001b[32m    270\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return model outputs and mask coefficients if training, otherwise return outputs and mask coefficients.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     p = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mproto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# mask protos\u001b[39;00m\n\u001b[32m    272\u001b[39m     bs = p.shape[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# batch size\u001b[39;00m\n\u001b[32m    274\u001b[39m     mc = torch.cat([\u001b[38;5;28mself\u001b[39m.cv4[i](x[i]).view(bs, \u001b[38;5;28mself\u001b[39m.nm, -\u001b[32m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.nl)], \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# mask coefficients\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:105\u001b[39m, in \u001b[36mProto.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Perform a forward pass through layers using an upsampled input image.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cv3(\u001b[38;5;28mself\u001b[39m.cv2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mupsample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:1161\u001b[39m, in \u001b[36mConvTranspose2d.forward\u001b[39m\u001b[34m(self, input, output_size)\u001b[39m\n\u001b[32m   1150\u001b[39m num_spatial_dims = \u001b[32m2\u001b[39m\n\u001b[32m   1151\u001b[39m output_padding = \u001b[38;5;28mself\u001b[39m._output_padding(\n\u001b[32m   1152\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1153\u001b[39m     output_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1158\u001b[39m     \u001b[38;5;28mself\u001b[39m.dilation,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   1159\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1161\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4bcf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7f019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# упрощённый фрагмент\n",
    "if hasattr(res, 'masks') and res.masks is not None:\n",
    "    im_with_masks = res.masks.render(res.orig_img)  # иногда работает напрямую\n",
    "    frame = im_with_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f2e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(gt_mask, pred_mask):\n",
    "    gt = gt_mask.astype(bool)\n",
    "    pr = pred_mask.astype(bool)\n",
    "    inter = (gt & pr).sum()\n",
    "    return 2*inter / (gt.sum() + pr.sum() + 1e-6)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mai (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
