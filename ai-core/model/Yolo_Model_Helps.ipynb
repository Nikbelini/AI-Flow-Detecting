{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c3bd6b",
   "metadata": {},
   "source": [
    "# Предобученная модель YOLO для детектинга людей  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2dd971",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'runs\\\\train\\\\yolov10n_bus_stop_finetuned8\\\\weights\\\\best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Загрузка предобученной модели YOLOv8 (можно 'yolov8n.pt', 'yolov8s.pt' и т.д.)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model = \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mruns/train/yolov10n_bus_stop_finetuned8/weights/best.pt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\AI-Flow-Detecting\\ai-core\\.venv\\Lib\\site-packages\\ultralytics\\models\\yolo\\model.py:79\u001b[39m, in \u001b[36mYOLO.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m = new_instance.\u001b[34m__dict__\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     78\u001b[39m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mRTDETR\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.model[-\u001b[32m1\u001b[39m]._get_name():  \u001b[38;5;66;03m# if RTDETR head\u001b[39;00m\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RTDETR\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\AI-Flow-Detecting\\ai-core\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:151\u001b[39m, in \u001b[36mModel.__init__\u001b[39m\u001b[34m(self, model, task, verbose)\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28mself\u001b[39m._new(model, task=task, verbose=verbose)\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Delete super().training for accessing self.model.training\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\AI-Flow-Detecting\\ai-core\\.venv\\Lib\\site-packages\\ultralytics\\engine\\model.py:295\u001b[39m, in \u001b[36mModel._load\u001b[39m\u001b[34m(self, weights, task)\u001b[39m\n\u001b[32m    292\u001b[39m weights = checks.check_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(weights).rpartition(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.ckpt = \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    296\u001b[39m     \u001b[38;5;28mself\u001b[39m.task = \u001b[38;5;28mself\u001b[39m.model.task\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m.overrides = \u001b[38;5;28mself\u001b[39m.model.args = \u001b[38;5;28mself\u001b[39m._reset_ckpt_args(\u001b[38;5;28mself\u001b[39m.model.args)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\AI-Flow-Detecting\\ai-core\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1549\u001b[39m, in \u001b[36mattempt_load_one_weight\u001b[39m\u001b[34m(weight, device, inplace, fuse)\u001b[39m\n\u001b[32m   1535\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mattempt_load_one_weight\u001b[39m(weight, device=\u001b[38;5;28;01mNone\u001b[39;00m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m, fuse=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1536\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1537\u001b[39m \u001b[33;03m    Load a single model weights.\u001b[39;00m\n\u001b[32m   1538\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1547\u001b[39m \u001b[33;03m        ckpt (dict): Model checkpoint dictionary.\u001b[39;00m\n\u001b[32m   1548\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m     ckpt, weight = \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[32m   1550\u001b[39m     args = {**DEFAULT_CFG_DICT, **(ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mtrain_args\u001b[39m\u001b[33m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[32m   1551\u001b[39m     model = (ckpt.get(\u001b[33m\"\u001b[39m\u001b[33mema\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m]).to(device).float()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\AI-Flow-Detecting\\ai-core\\.venv\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:1447\u001b[39m, in \u001b[36mtorch_safe_load\u001b[39m\u001b[34m(weight, safe_only)\u001b[39m\n\u001b[32m   1445\u001b[39m                 ckpt = torch_load(f, pickle_module=safe_pickle)\n\u001b[32m   1446\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1447\u001b[39m             ckpt = \u001b[43mtorch_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m e.name == \u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\AI-Flow-Detecting\\ai-core\\.venv\\Lib\\site-packages\\ultralytics\\utils\\patches.py:118\u001b[39m, in \u001b[36mtorch_load\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TORCH_1_13 \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n\u001b[32m    116\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mweights_only\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\AI-Flow-Detecting\\ai-core\\.venv\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1317\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1321\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1323\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1324\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\AI-Flow-Detecting\\ai-core\\.venv\\Lib\\site-packages\\torch\\serialization.py:659\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    661\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\AI-Flow-Detecting\\ai-core\\.venv\\Lib\\site-packages\\torch\\serialization.py:640\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'runs\\\\train\\\\yolov10n_bus_stop_finetuned8\\\\weights\\\\best.pt'"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Загрузка предобученной модели YOLOv8 (можно 'yolov8n.pt', 'yolov8s.pt' и т.д.)\n",
    "model = YOLO(\"runs/train/yolov10n_bus_stop_finetuned8/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccd8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 6 persons, 137.8ms\n",
      "Speed: 12.2ms preprocess, 137.8ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Количество людей на фото: 6\n"
     ]
    }
   ],
   "source": [
    "# Загрузка изображения\n",
    "#image = cv2.imread('2.png')\n",
    "image = cv2.imread('E://AIM/AI-Flow-Detecting/ai-core/data/2025-08-08_11-29-04.png')\n",
    "\n",
    "# Детекция людей (класс '0' в COCO = человек)\n",
    "results = model(image, classes=[0])  \n",
    "\n",
    "# Визуализация результатов\n",
    "annotated_image = results[0].plot()  # Рисует bounding boxes\n",
    "cv2.imshow('Detected People', annotated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Подсчёт количества людей\n",
    "people_count = len(results[0].boxes)\n",
    "print(f\"Количество людей на фото: {people_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747fe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Видеопоток запущен. Детекция людей через YOLOv8...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Детекция ТОЛЬКО людей (класс 0 в COCO)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m results = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Наносим bounding boxes и метки\u001b[39;00m\n\u001b[32m     30\u001b[39m annotated_frame = results[\u001b[32m0\u001b[39m].plot()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\model.py:555\u001b[39m, in \u001b[36mModel.predict\u001b[39m\u001b[34m(self, source, stream, predictor, **kwargs)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.predictor, \u001b[33m\"\u001b[39m\u001b[33mset_prompts\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.predictor.set_prompts(prompts)\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predictor.predict_cli(source=source) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m=\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:227\u001b[39m, in \u001b[36mBasePredictor.__call__\u001b[39m\u001b[34m(self, source, model, stream, *args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream_inference(source, model, *args, **kwargs)\n\u001b[32m    226\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m227\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\utils\\_contextlib.py:38\u001b[39m, in \u001b[36m_wrap_generator.<locals>.generator_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         response = \u001b[43mgen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:330\u001b[39m, in \u001b[36mBasePredictor.stream_inference\u001b[39m\u001b[34m(self, source, model, *args, **kwargs)\u001b[39m\n\u001b[32m    328\u001b[39m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     preds = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.embed:\n\u001b[32m    332\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch.Tensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:182\u001b[39m, in \u001b[36mBasePredictor.inference\u001b[39m\u001b[34m(self, im, *args, **kwargs)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[32m    177\u001b[39m visualize = (\n\u001b[32m    178\u001b[39m     increment_path(\u001b[38;5;28mself\u001b[39m.save_dir / Path(\u001b[38;5;28mself\u001b[39m.batch[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).stem, mkdir=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.visualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.source_type.tensor)\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    181\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:644\u001b[39m, in \u001b[36mAutoBackend.forward\u001b[39m\u001b[34m(self, im, augment, visualize, embed, **kwargs)\u001b[39m\n\u001b[32m    642\u001b[39m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.nn_module:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     y = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m=\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.jit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:139\u001b[39m, in \u001b[36mBaseModel.forward\u001b[39m\u001b[34m(self, x, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loss(x, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:157\u001b[39m, in \u001b[36mBaseModel.predict\u001b[39m\u001b[34m(self, x, profile, visualize, augment, embed)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._predict_augment(x)\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:180\u001b[39m, in \u001b[36mBaseModel._predict_once\u001b[39m\u001b[34m(self, x, profile, visualize, embed)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m._profile_one_layer(m, x, dt)\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m x = \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[32m    181\u001b[39m y.append(x \u001b[38;5;28;01mif\u001b[39;00m m.i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\modules\\block.py:319\u001b[39m, in \u001b[36mC2f.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    317\u001b[39m y = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.cv1(x).chunk(\u001b[32m2\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    318\u001b[39m y.extend(m(y[-\u001b[32m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.m)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:92\u001b[39m, in \u001b[36mConv.forward_fuse\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     83\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    Apply convolution and activation without batch normalization.\u001b[39;00m\n\u001b[32m     85\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     90\u001b[39m \u001b[33;03m        (torch.Tensor): Output tensor.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:434\u001b[39m, in \u001b[36mSiLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\torch\\nn\\functional.py:2374\u001b[39m, in \u001b[36msilu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   2372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(silu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace=inplace)\n\u001b[32m   2373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m-> \u001b[39m\u001b[32m2374\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43msilu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2375\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.silu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mПри выполнении кода в текущей ячейке или предыдущей ячейке ядро аварийно завершило работу. \n",
      "\u001b[1;31mПроверьте код в ячейках, чтобы определить возможную причину сбоя. \n",
      "\u001b[1;31mЩелкните <a href='https://aka.ms/vscodeJupyterKernelCrash'>здесь</a>, чтобы получить дополнительные сведения. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Загрузка модели YOLOv8 (nano — быстро, подходит для реального времени)\n",
    "model = YOLO('yolov10n.pt')\n",
    "\n",
    "# URL видеопотока (убраны лишние пробелы в конце!)\n",
    "stream_url = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"\n",
    "\n",
    "# Открываем видеопоток\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Ошибка: Не удалось открыть видеопоток. Проверь URL и соединение.\")\n",
    "    exit()\n",
    "\n",
    "print(\"✅ Видеопоток запущен. Детекция людей через YOLOv8...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"⚠️ Не удалось получить кадр. Поток может быть разорван.\")\n",
    "        break\n",
    "\n",
    "    # Детекция ТОЛЬКО людей (класс 0 в COCO)\n",
    "    results = model.predict(frame, classes=[0], conf=0.5, verbose=False)\n",
    "\n",
    "    # Наносим bounding boxes и метки\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Подсчёт обнаруженных людей\n",
    "    people_count = len(results[0].boxes)\n",
    "    cv2.putText(annotated_frame, f'Людей: {people_count}', (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Показываем кадр\n",
    "    cv2.imshow('YOLOv8 — Детекция людей в реальном времени', annotated_frame)\n",
    "\n",
    "    # Выход по клавише 'q' или пробелу\n",
    "    if cv2.waitKey(1) & 0xFF in [ord('q'), ord(' ')]:\n",
    "        break\n",
    "\n",
    "# Освобождение ресурсов\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"⏹️ Работа завершена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9ee499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Display image and videos\n",
    "import IPython\n",
    "from IPython.display import Video, display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import urllib.request \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a612e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сбор кадров начат... Нажми 'q' для остановки.\n",
      "Сохранён кадр 0\n",
      "Сохранён кадр 60\n",
      "Сохранён кадр 120\n",
      "Сохранён кадр 180\n",
      "Сохранён кадр 240\n",
      "Сохранён кадр 300\n",
      "Сохранён кадр 360\n",
      "Сохранён кадр 420\n",
      "Сохранён кадр 480\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/images\", exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(\"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\")\n",
    "count = 0\n",
    "\n",
    "print(\"Сбор кадров начат... Нажми 'q' для остановки.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        if count % 60 == 0:  # один кадр каждые 2 сек (30 FPS)\n",
    "            cv2.imwrite(f\"data/images/frame_{count}.jpg\", frame)\n",
    "            print(f\"Сохранён кадр {count}\")\n",
    "        count += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or count > 500:  # 500 кадров\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05094132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется: cpu\n",
      "Ultralytics 8.3.176  Python-3.12.6 torch-2.8.0+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=0.25, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.5, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov10n_bus_stop_finetuned, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\train\\yolov10n_bus_stop_finetuned, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset 'data.yaml' error  'data.yaml' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:607\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.data.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myaml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myml\u001b[39m\u001b[33m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    602\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetect\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    603\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msegment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    604\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    605\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    606\u001b[39m }:\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     data = \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\data\\utils.py:402\u001b[39m, in \u001b[36mcheck_det_dataset\u001b[39m\u001b[34m(dataset, autodownload)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03mDownload, verify, and/or unzip a dataset if not found locally.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    400\u001b[39m \u001b[33;03m    (Dict[str, Any]): Parsed dataset information and paths.\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m file = \u001b[43mcheck_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# Download (optional)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\checks.py:568\u001b[39m, in \u001b[36mcheck_file\u001b[39m\u001b[34m(file, suffix, download, download_dir, hard)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hard:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: 'data.yaml' does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolov8n.pt\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# можно yolov8s.pt для точности\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Обучение\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# см. ниже\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myolov10n_bus_stop_finetuned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAdamW\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43miou\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mruns/train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Сохранение\u001b[39;00m\n\u001b[32m     30\u001b[39m model.save(\u001b[33m'\u001b[39m\u001b[33mmodels/yolov10n_best.pt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\model.py:793\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    791\u001b[39m     args[\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.ckpt_path\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrainer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[32m    795\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:153\u001b[39m, in \u001b[36mBaseTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m.model = check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m.args.model)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m.ema = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:611\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    609\u001b[39m             \u001b[38;5;28mself\u001b[39m.args.data = data[\u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m.args.data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m error ❌ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.single_cls:\n\u001b[32m    613\u001b[39m     LOGGER.info(\u001b[33m\"\u001b[39m\u001b[33mOverriding class names with single class.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset 'data.yaml' error  'data.yaml' does not exist"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Выбираем устройство\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Используется: {device}\")\n",
    "\n",
    "# Загружаем предобученную модель\n",
    "model = YOLO('yolov8n.pt')  # можно yolov8s.pt для точности\n",
    "\n",
    "# Обучение\n",
    "model.train(\n",
    "    data='data.yaml',        # см. ниже\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=device,\n",
    "    name='yolov10n_bus_stop_finetuned',\n",
    "    augment=True,\n",
    "    patience=15,\n",
    "    optimizer='AdamW',\n",
    "    lr0=1e-3,\n",
    "    iou=0.5,\n",
    "    conf=0.25,\n",
    "    project='runs/train'\n",
    ")\n",
    "\n",
    "# Сохранение\n",
    "model.save('models/yolov10n_best.pt')\n",
    "print(\"✅ Модель дообучена и сохранена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect_on_stream.py\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# -------------------------------\n",
    "# ### Конфигурации\n",
    "# -------------------------------\n",
    "MODEL_PATH = 'runs/train/yolov8n_bus_stop_finetuned/weights/best.pt'  # твоя дообученная модель\n",
    "STREAM_URL = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"\n",
    "CONF_LEVEL = 0.6\n",
    "SCALE_PERCENT = 100\n",
    "VIDEO_NAME = \"result.mp4\"\n",
    "ROI_POINTS = np.array([(1250, 400), (750, 400), (700, 800), (1200, 800)], np.int32)\n",
    "ALPHA = 0.1\n",
    "PATIENCE = 100\n",
    "FRAME_MAX = 5\n",
    "THR_CENTERS = 25\n",
    "\n",
    "# -------------------------------\n",
    "# Загрузка модели\n",
    "# -------------------------------\n",
    "model = YOLO(MODEL_PATH)\n",
    "dict_classes = model.model.names\n",
    "\n",
    "# -------------------------------\n",
    "# Открываем поток\n",
    "# -------------------------------\n",
    "cap = cv2.VideoCapture(STREAM_URL)\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Не удалось открыть поток\")\n",
    "    exit()\n",
    "\n",
    "# Параметры видео\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "if SCALE_PERCENT != 100:\n",
    "    width = int(width * SCALE_PERCENT / 100)\n",
    "    height = int(height * SCALE_PERCENT / 100)\n",
    "\n",
    "# -------------------------------\n",
    "# VideoWriter\n",
    "# -------------------------------\n",
    "tmp_out = \"tmp_result.mp4\"\n",
    "output_out = \"rep_result.mp4\"\n",
    "\n",
    "if os.path.exists(tmp_out): os.remove(tmp_out)\n",
    "if os.path.exists(output_out): os.remove(output_out)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(tmp_out, fourcc, fps, (width, height))\n",
    "\n",
    "# -------------------------------\n",
    "# Вспомогательные функции\n",
    "# -------------------------------\n",
    "def resize_frame(frame, scale):\n",
    "    if scale == 100: return frame\n",
    "    w = int(frame.shape[1] * scale / 100)\n",
    "    h = int(frame.shape[0] * scale / 100)\n",
    "    return cv2.resize(frame, (w, h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def update_tracking(centers_old, center, thr, last_key, frame_idx, frame_max):\n",
    "    best_id = None\n",
    "    min_dist = float('inf')\n",
    "    for obj_id, history in centers_old.items():\n",
    "        last_center = list(history.keys())[-1]\n",
    "        dist = np.linalg.norm(np.array(last_center) - np.array(center))\n",
    "        if dist < thr and dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_id = obj_id\n",
    "\n",
    "    if best_id is not None:\n",
    "        centers_old[best_id][center] = frame_idx\n",
    "        return centers_old, best_id, False, best_id\n",
    "    else:\n",
    "        new_id = f\"p_{len(centers_old)}\"\n",
    "        centers_old[new_id] = {center: frame_idx}\n",
    "        return centers_old, new_id, True, new_id\n",
    "\n",
    "def filter_tracks(centers_old, patience):\n",
    "    current_frame = max([max(t.values()) for t in centers_old.values()] + [0])\n",
    "    to_remove = [k for k, v in centers_old.items() if current_frame - max(v.values()) > patience]\n",
    "    for k in to_remove:\n",
    "        del centers_old[k]\n",
    "    return centers_old\n",
    "\n",
    "# -------------------------------\n",
    "# Основной цикл\n",
    "# -------------------------------\n",
    "centers_old = {}\n",
    "total_count = 0\n",
    "frame_idx = 0\n",
    "\n",
    "print(\"✅ Обработка начата. Нажмите 'q' для остановки.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"⚠️ Кадр не получен. Переподключение...\")\n",
    "        cap.release()\n",
    "        cap = cv2.VideoCapture(STREAM_URL)\n",
    "        continue\n",
    "\n",
    "    frame_idx += 1\n",
    "    frame = resize_frame(frame, SCALE_PERCENT)\n",
    "    display_frame = frame.copy()\n",
    "\n",
    "    # --- ROI ---\n",
    "    overlay = display_frame.copy()\n",
    "    cv2.polylines(overlay, [ROI_POINTS], True, (255, 0, 0), 2)\n",
    "    cv2.fillPoly(overlay, [ROI_POINTS], (255, 0, 0))\n",
    "    cv2.addWeighted(overlay, ALPHA, display_frame, 1 - ALPHA, 0, display_frame)\n",
    "\n",
    "    # Вырезаем ROI\n",
    "    x_min, y_min = ROI_POINTS[:, 0].min(), ROI_POINTS[:, 1].min()\n",
    "    x_max, y_max = ROI_POINTS[:, 0].max(), ROI_POINTS[:, 1].max()\n",
    "    roi_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    # --- Детекция ---\n",
    "    results = model(roi_frame, conf=CONF_LEVEL, classes=[0], verbose=False)\n",
    "    if len(results[0].boxes) == 0:\n",
    "        detections = pd.DataFrame(columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf'])\n",
    "    else:\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        conf = results[0].boxes.conf.cpu().numpy()\n",
    "        detections = pd.DataFrame(np.column_stack([boxes, conf]),\n",
    "                                  columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf'])\n",
    "\n",
    "    # --- Обработка каждого человека ---\n",
    "    for _, row in detections.iterrows():\n",
    "        xmin, ymin, xmax, ymax, conf_val = map(int, row)\n",
    "        cx = (xmin + xmax) // 2 + x_min\n",
    "        cy = (ymin + ymax) // 2 + y_min\n",
    "\n",
    "        centers_old, obj_id, is_new, _ = update_tracking(\n",
    "            centers_old, (cx, cy), THR_CENTERS, None, frame_idx, FRAME_MAX\n",
    "        )\n",
    "        total_count += is_new\n",
    "\n",
    "        # Рисуем\n",
    "        cv2.rectangle(display_frame, (xmin + x_min, ymin + y_min), (xmax + x_min, ymax + y_min), (0, 255, 0), 2)\n",
    "        cv2.circle(display_frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "        cv2.putText(display_frame, f\"{obj_id} ({conf_val:.2f})\", (xmin + x_min, ymin + y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "\n",
    "    # Фильтрация\n",
    "    centers_old = filter_tracks(centers_old, PATIENCE)\n",
    "\n",
    "    # Счётчик\n",
    "    cv2.putText(display_frame, f\"Людей в зоне: {total_count}\", (30, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2)\n",
    "\n",
    "    # Запись и отображение\n",
    "    out.write(display_frame)\n",
    "    cv2.imshow(\"YOLOv8 — Пассажиропоток\", display_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# -------------------------------\n",
    "# Финализация\n",
    "# -------------------------------\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Перекодировка\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-i\", tmp_out,\n",
    "    \"-crf\", \"18\", \"-preset\", \"veryfast\",\n",
    "    \"-vcodec\", \"libx264\", output_out\n",
    "], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "os.remove(tmp_out)\n",
    "print(f\"✅ Видео сохранено: {output_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27cb533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Original Dim: 1920x1080, FPS: 25.0\n",
      "✅ Начало обработки видеопотока. Нажмите 'q' для остановки.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'obj_id' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 190\u001b[39m\n\u001b[32m    187\u001b[39m center_x += x_min\n\u001b[32m    188\u001b[39m center_y += y_min\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m centers_old, obj_id, is_new, lastKey = \u001b[43mupdate_tracking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenters_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenter_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthr_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlastKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_max\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m count_p += is_new\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Рисуем bounding box и центр\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mupdate_tracking\u001b[39m\u001b[34m(centers_old, current_center, threshold, lastKey, frame_idx, frame_max)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m centers_old, best_key, \u001b[38;5;28;01mFalse\u001b[39;00m, best_key\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# Новый объект\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     new_id = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mperson_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mobj_id\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m     centers_old[new_id] = {(center_x, center_y): frame_idx}\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m centers_old, new_id, \u001b[38;5;28;01mTrue\u001b[39;00m, new_id\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'obj_id' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# ### Configurations\n",
    "# -------------------------------\n",
    "verbose = False\n",
    "scale_percent = 100  # масштабирование кадра\n",
    "conf_level = 0.8     # порог уверенности\n",
    "thr_centers = 20     # порог для сопоставления центров\n",
    "frame_max = 5        # макс. кадров до \"потери\" объекта\n",
    "patience = 100       # макс. длина истории треков\n",
    "alpha = 0.1          # прозрачность ROI\n",
    "video_name = 'result.mp4'\n",
    "\n",
    "# URL видеопотока\n",
    "stream_url = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"\n",
    "\n",
    "# Загрузка модели\n",
    "model = YOLO('yolov8n.pt')  # или yolov10n.pt\n",
    "\n",
    "# Классы: только люди\n",
    "class_IDS = [0]\n",
    "dict_classes = {0: 'person'}\n",
    "\n",
    "# Вспомогательные переменные\n",
    "centers_old = {}\n",
    "obj_id = 0\n",
    "count_p = 0\n",
    "lastKey = ''\n",
    "\n",
    "# -------------------------------\n",
    "# Открываем видеопоток\n",
    "# -------------------------------\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Ошибка: Не удалось открыть видеопоток.\")\n",
    "    exit()\n",
    "\n",
    "# Параметры видео\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(f'[INFO] - Original Dim: {width}x{height}, FPS: {fps}')\n",
    "\n",
    "# Масштабирование (если нужно)\n",
    "if scale_percent != 100:\n",
    "    width = int(width * scale_percent / 100)\n",
    "    height = int(height * scale_percent / 100)\n",
    "    print(f'[INFO] - Dim Scaled: {width}x{height}')\n",
    "\n",
    "# Функция для изменения размера кадра\n",
    "def risize_frame(frame, scale_percent):\n",
    "    if scale_percent == 100:\n",
    "        return frame\n",
    "    width_new = int(frame.shape[1] * scale_percent / 100)\n",
    "    height_new = int(frame.shape[0] * scale_percent / 100)\n",
    "    return cv2.resize(frame, (width_new, height_new), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# -------------------------------\n",
    "# Настройка записи видео\n",
    "# -------------------------------\n",
    "output_path = \"rep_\" + video_name\n",
    "tmp_output_path = \"tmp_\" + output_path\n",
    "\n",
    "# Удаляем старые файлы\n",
    "for f in [tmp_output_path, output_path]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "VIDEO_CODEC = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "output_video = cv2.VideoWriter(tmp_output_path, VIDEO_CODEC, fps, (width, height))\n",
    "\n",
    "if not output_video.isOpened():\n",
    "    print(\"❌ Ошибка: Не удалось инициализировать VideoWriter.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# -------------------------------\n",
    "# ROI: область интереса (остановка)\n",
    "# -------------------------------\n",
    "def get_roi_points():\n",
    "    # Пример: треугольник или трапеция на остановке\n",
    "    # Адаптируй под свою камеру!\n",
    "    return [np.array([(1250, 400), (750, 400), (700, 800), (1200, 800)], np.int32)]\n",
    "\n",
    "# -------------------------------\n",
    "# Функция обновления трекинга\n",
    "# -------------------------------\n",
    "def update_tracking(centers_old, current_center, threshold, lastKey, frame_idx, frame_max):\n",
    "    center_x, center_y = current_center\n",
    "    is_new = False\n",
    "    min_dist = float('inf')\n",
    "    best_key = None\n",
    "\n",
    "    for obj_id, centers in centers_old.items():\n",
    "        # Последний известный центр\n",
    "        last_center = list(centers.keys())[-1]\n",
    "        cx, cy = last_center\n",
    "        dist = np.sqrt((cx - center_x)**2 + (cy - center_y)**2)\n",
    "        if dist < min_dist and dist < threshold:\n",
    "            min_dist = dist\n",
    "            best_key = obj_id\n",
    "\n",
    "    if best_key is not None:\n",
    "        # Обновляем трек\n",
    "        centers_old[best_key][(center_x, center_y)] = frame_idx\n",
    "        return centers_old, best_key, False, best_key\n",
    "    else:\n",
    "        # Новый объект\n",
    "        new_id = f\"person_{obj_id}\"\n",
    "        centers_old[new_id] = {(center_x, center_y): frame_idx}\n",
    "        return centers_old, new_id, True, new_id\n",
    "\n",
    "# -------------------------------\n",
    "# Фильтрация старых треков\n",
    "# -------------------------------\n",
    "def filter_tracks(centers_old, patience):\n",
    "    current_frame = max([max(frames.values()) for frames in centers_old.values()] + [0]) if centers_old else 0\n",
    "    keys_to_remove = []\n",
    "    for obj_id, centers in centers_old.items():\n",
    "        last_seen = max(centers.values())\n",
    "        if current_frame - last_seen > patience:\n",
    "            keys_to_remove.append(obj_id)\n",
    "    for k in keys_to_remove:\n",
    "        del centers_old[k]\n",
    "    return centers_old\n",
    "\n",
    "# -------------------------------\n",
    "# Основной цикл обработки потока\n",
    "# -------------------------------\n",
    "print(\"✅ Начало обработки видеопотока. Нажмите 'q' для остановки.\")\n",
    "\n",
    "frame_count = 0\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"⚠️ Не удалось получить кадр. Попытка переподключения...\")\n",
    "            time.sleep(2)\n",
    "            cap.release()\n",
    "            cap = cv2.VideoCapture(stream_url)\n",
    "            continue\n",
    "\n",
    "        frame_count += 1\n",
    "        frame = risize_frame(frame, scale_percent)\n",
    "        overlay = frame.copy()\n",
    "\n",
    "        # --- ROI ---\n",
    "        area_roi = get_roi_points()\n",
    "        cv2.polylines(overlay, pts=area_roi, isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        cv2.fillPoly(overlay, area_roi, (255, 0, 0))\n",
    "        frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "        # Вырезаем ROI\n",
    "        x_min = min([p[0] for p in area_roi[0]])\n",
    "        x_max = max([p[0] for p in area_roi[0]])\n",
    "        y_min = min([p[1] for p in area_roi[0]])\n",
    "        y_max = max([p[1] for p in area_roi[0]])\n",
    "        roi_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        # --- Детекция ---\n",
    "        results = model.predict(roi_frame, conf=conf_level, classes=class_IDS, verbose=False)\n",
    "        if len(results[0].boxes) == 0:\n",
    "            positions_frame = pd.DataFrame(columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "        else:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "            conf = results[0].boxes.conf.cpu().numpy()\n",
    "            classes = results[0].boxes.cls.cpu().numpy()\n",
    "            positions_frame = pd.DataFrame(np.column_stack([boxes, conf, classes]),\n",
    "                                           columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "\n",
    "        # --- Обработка каждого человека ---\n",
    "        for idx, row in positions_frame.iterrows():\n",
    "            xmin, ymin, xmax, ymax, conf_val, cls = map(int, row[:6])\n",
    "            center_x = (xmin + xmax) // 2\n",
    "            center_y = (ymin + ymax) // 2\n",
    "\n",
    "            # Смещение центра в глобальные координаты\n",
    "            center_x += x_min\n",
    "            center_y += y_min\n",
    "\n",
    "            centers_old, obj_id, is_new, lastKey = update_tracking(\n",
    "                centers_old, (center_x, center_y), thr_centers, lastKey, frame_count, frame_max\n",
    "            )\n",
    "            count_p += is_new\n",
    "\n",
    "            # Рисуем bounding box и центр\n",
    "            cv2.rectangle(frame, (xmin + x_min, ymin + y_min), (xmax + x_min, ymax + y_min), (0, 0, 255), 2)\n",
    "            cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"{obj_id}:{conf_val:.2f}\", (xmin + x_min, ymin + y_min - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)\n",
    "\n",
    "        # --- Фильтрация старых треков ---\n",
    "        centers_old = filter_tracks(centers_old, patience)\n",
    "\n",
    "        # --- Отображение счётчика ---\n",
    "        cv2.putText(frame, f'People in ROI: {count_p}', (30, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 2)\n",
    "\n",
    "        # --- Запись кадра ---\n",
    "        output_video.write(frame)\n",
    "\n",
    "        # --- Показываем результат ---\n",
    "        cv2.imshow(\"Live Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF in [ord('q'), ord(' ')]:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"⏹️ Остановлено пользователем.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Финализация: перекодировка видео\n",
    "# -------------------------------\n",
    "output_video.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"🔁 Перекодировка видео для совместимости...\")\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-i\", tmp_output_path,\n",
    "    \"-crf\", \"18\",\n",
    "    \"-preset\", \"veryfast\",\n",
    "    \"-hide_banner\", \"-loglevel\", \"error\",\n",
    "    \"-vcodec\", \"libx264\", output_path\n",
    "], check=True)\n",
    "\n",
    "os.remove(tmp_output_path)\n",
    "print(f\"✅ Видео сохранено: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8495eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking samples of processed frames\n",
    "for i in [62,63, 64, 65, 66]:\n",
    "    plt.figure(figsize =( 14, 10))\n",
    "    plt.imshow(frames_list[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output video result\n",
    "frac = 0.7 \n",
    "Video(data='rep_result.mp4', embed=True, height=int(720 * frac), width=int(1280 * frac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
