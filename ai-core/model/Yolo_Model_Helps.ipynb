{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c3bd6b",
   "metadata": {},
   "source": [
    "# –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å YOLO –¥–ª—è –¥–µ—Ç–µ–∫—Ç–∏–Ω–≥–∞ –ª—é–¥–µ–π  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2dd971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ YOLOv8 (–º–æ–∂–Ω–æ 'yolov8n.pt', 'yolov8s.pt' –∏ —Ç.–¥.)\n",
    "model = YOLO('yolov10n.pt')  # nano-–≤–µ—Ä—Å–∏—è (–±—ã—Å—Ç—Ä–∞—è, –Ω–æ –º–µ–Ω–µ–µ —Ç–æ—á–Ω–∞—è)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ccd8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 100.7ms\n",
      "Speed: 4.3ms preprocess, 100.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—é–¥–µ–π –Ω–∞ —Ñ–æ—Ç–æ: 3\n"
     ]
    }
   ],
   "source": [
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "#image = cv2.imread('2.png')\n",
    "image = cv2.imread('E://AIM/AI-Flow-Detecting/ai-core/data/2025-08-08_11-29-04.png')\n",
    "\n",
    "# –î–µ—Ç–µ–∫—Ü–∏—è –ª—é–¥–µ–π (–∫–ª–∞—Å—Å '0' –≤ COCO = —á–µ–ª–æ–≤–µ–∫)\n",
    "results = model(image, classes=[0])  \n",
    "\n",
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "annotated_image = results[0].plot()  # –†–∏—Å—É–µ—Ç bounding boxes\n",
    "cv2.imshow('Detected People', annotated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# –ü–æ–¥—Å—á—ë—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª—é–¥–µ–π\n",
    "people_count = len(results[0].boxes)\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—é–¥–µ–π –Ω–∞ —Ñ–æ—Ç–æ: {people_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2747fe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –í–∏–¥–µ–æ–ø–æ—Ç–æ–∫ –∑–∞–ø—É—â–µ–Ω. –î–µ—Ç–µ–∫—Ü–∏—è –ª—é–¥–µ–π —á–µ—Ä–µ–∑ YOLOv8...\n",
      "‚èπÔ∏è –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ YOLOv8 (nano ‚Äî –±—ã—Å—Ç—Ä–æ, –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏)\n",
    "model = YOLO('yolov10n.pt')\n",
    "\n",
    "# URL –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞ (—É–±—Ä–∞–Ω—ã –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –∫–æ–Ω—Ü–µ!)\n",
    "stream_url = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"\n",
    "\n",
    "# –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫. –ü—Ä–æ–≤–µ—Ä—å URL –∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ.\")\n",
    "    exit()\n",
    "\n",
    "print(\"‚úÖ –í–∏–¥–µ–æ–ø–æ—Ç–æ–∫ –∑–∞–ø—É—â–µ–Ω. –î–µ—Ç–µ–∫—Ü–∏—è –ª—é–¥–µ–π —á–µ—Ä–µ–∑ YOLOv8...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä. –ü–æ—Ç–æ–∫ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–∞–∑–æ—Ä–≤–∞–Ω.\")\n",
    "        break\n",
    "\n",
    "    # –î–µ—Ç–µ–∫—Ü–∏—è –¢–û–õ–¨–ö–û –ª—é–¥–µ–π (–∫–ª–∞—Å—Å 0 –≤ COCO)\n",
    "    results = model.predict(frame, classes=[0], conf=0.5, verbose=False)\n",
    "\n",
    "    # –ù–∞–Ω–æ—Å–∏–º bounding boxes –∏ –º–µ—Ç–∫–∏\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # –ü–æ–¥—Å—á—ë—Ç –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –ª—é–¥–µ–π\n",
    "    people_count = len(results[0].boxes)\n",
    "    cv2.putText(annotated_frame, f'–õ—é–¥–µ–π: {people_count}', (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–∞–¥—Ä\n",
    "    cv2.imshow('YOLOv8 ‚Äî –î–µ—Ç–µ–∫—Ü–∏—è –ª—é–¥–µ–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏', annotated_frame)\n",
    "\n",
    "    # –í—ã—Ö–æ–¥ –ø–æ –∫–ª–∞–≤–∏—à–µ 'q' –∏–ª–∏ –ø—Ä–æ–±–µ–ª—É\n",
    "    if cv2.waitKey(1) & 0xFF in [ord('q'), ord(' ')]:\n",
    "        break\n",
    "\n",
    "# –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"‚èπÔ∏è –†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9ee499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Display image and videos\n",
    "import IPython\n",
    "from IPython.display import Video, display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import urllib.request \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a612e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–°–±–æ—Ä –∫–∞–¥—Ä–æ–≤ –Ω–∞—á–∞—Ç... –ù–∞–∂–º–∏ 'q' –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.\n",
      "–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä 0\n",
      "–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä 60\n",
      "–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä 120\n",
      "–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä 180\n",
      "–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä 240\n",
      "–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä 300\n",
      "–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä 360\n",
      "–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä 420\n",
      "–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä 480\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/images\", exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(\"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\")\n",
    "count = 0\n",
    "\n",
    "print(\"–°–±–æ—Ä –∫–∞–¥—Ä–æ–≤ –Ω–∞—á–∞—Ç... –ù–∞–∂–º–∏ 'q' –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        if count % 60 == 0:  # –æ–¥–∏–Ω –∫–∞–¥—Ä –∫–∞–∂–¥—ã–µ 2 —Å–µ–∫ (30 FPS)\n",
    "            cv2.imwrite(f\"data/images/frame_{count}.jpg\", frame)\n",
    "            print(f\"–°–æ—Ö—Ä–∞–Ω—ë–Ω –∫–∞–¥—Ä {count}\")\n",
    "        count += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or count > 500:  # 500 –∫–∞–¥—Ä–æ–≤\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05094132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: cpu\n",
      "Ultralytics 8.3.176  Python-3.12.6 torch-2.8.0+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=0.25, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.5, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_bus_stop_finetuned, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\train\\yolov8n_bus_stop_finetuned, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset 'data.yaml' error  'data.yaml' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:607\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.data.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myaml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myml\u001b[39m\u001b[33m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    602\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetect\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    603\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msegment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    604\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    605\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    606\u001b[39m }:\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     data = \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\data\\utils.py:402\u001b[39m, in \u001b[36mcheck_det_dataset\u001b[39m\u001b[34m(dataset, autodownload)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03mDownload, verify, and/or unzip a dataset if not found locally.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    400\u001b[39m \u001b[33;03m    (Dict[str, Any]): Parsed dataset information and paths.\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m file = \u001b[43mcheck_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# Download (optional)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\checks.py:568\u001b[39m, in \u001b[36mcheck_file\u001b[39m\u001b[34m(file, suffix, download, download_dir, hard)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hard:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: 'data.yaml' does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolov8n.pt\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# –º–æ–∂–Ω–æ yolov8s.pt –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# –û–±—É—á–µ–Ω–∏–µ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# —Å–º. –Ω–∏–∂–µ\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myolov8n_bus_stop_finetuned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAdamW\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43miou\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mruns/train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\u001b[39;00m\n\u001b[32m     30\u001b[39m model.save(\u001b[33m'\u001b[39m\u001b[33mmodels/yolov8n_best.pt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\model.py:793\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    791\u001b[39m     args[\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.ckpt_path\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrainer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[32m    795\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:153\u001b[39m, in \u001b[36mBaseTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m.model = check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m.args.model)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m.ema = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:611\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    609\u001b[39m             \u001b[38;5;28mself\u001b[39m.args.data = data[\u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m.args.data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m error ‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.single_cls:\n\u001b[32m    613\u001b[39m     LOGGER.info(\u001b[33m\"\u001b[39m\u001b[33mOverriding class names with single class.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset 'data.yaml' error  'data.yaml' does not exist"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# –í—ã–±–∏—Ä–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {device}\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "model = YOLO('yolov8n.pt')  # –º–æ–∂–Ω–æ yolov8s.pt –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏\n",
    "\n",
    "# –û–±—É—á–µ–Ω–∏–µ\n",
    "model.train(\n",
    "    data='data.yaml',        # —Å–º. –Ω–∏–∂–µ\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=device,\n",
    "    name='yolov10n_bus_stop_finetuned',\n",
    "    augment=True,\n",
    "    patience=15,\n",
    "    optimizer='AdamW',\n",
    "    lr0=1e-3,\n",
    "    iou=0.5,\n",
    "    conf=0.25,\n",
    "    project='runs/train'\n",
    ")\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
    "model.save('models/yolov10n_best.pt')\n",
    "print(\"‚úÖ –ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect_on_stream.py\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# -------------------------------\n",
    "# ### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏\n",
    "# -------------------------------\n",
    "MODEL_PATH = 'runs/train/yolov8n_bus_stop_finetuned/weights/best.pt'  # —Ç–≤–æ—è –¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "STREAM_URL = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"\n",
    "CONF_LEVEL = 0.6\n",
    "SCALE_PERCENT = 100\n",
    "VIDEO_NAME = \"result.mp4\"\n",
    "ROI_POINTS = np.array([(1250, 400), (750, 400), (700, 800), (1200, 800)], np.int32)\n",
    "ALPHA = 0.1\n",
    "PATIENCE = 100\n",
    "FRAME_MAX = 5\n",
    "THR_CENTERS = 25\n",
    "\n",
    "# -------------------------------\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "# -------------------------------\n",
    "model = YOLO(MODEL_PATH)\n",
    "dict_classes = model.model.names\n",
    "\n",
    "# -------------------------------\n",
    "# –û—Ç–∫—Ä—ã–≤–∞–µ–º –ø–æ—Ç–æ–∫\n",
    "# -------------------------------\n",
    "cap = cv2.VideoCapture(STREAM_URL)\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –ø–æ—Ç–æ–∫\")\n",
    "    exit()\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "if SCALE_PERCENT != 100:\n",
    "    width = int(width * SCALE_PERCENT / 100)\n",
    "    height = int(height * SCALE_PERCENT / 100)\n",
    "\n",
    "# -------------------------------\n",
    "# VideoWriter\n",
    "# -------------------------------\n",
    "tmp_out = \"tmp_result.mp4\"\n",
    "output_out = \"rep_result.mp4\"\n",
    "\n",
    "if os.path.exists(tmp_out): os.remove(tmp_out)\n",
    "if os.path.exists(output_out): os.remove(output_out)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(tmp_out, fourcc, fps, (width, height))\n",
    "\n",
    "# -------------------------------\n",
    "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏\n",
    "# -------------------------------\n",
    "def resize_frame(frame, scale):\n",
    "    if scale == 100: return frame\n",
    "    w = int(frame.shape[1] * scale / 100)\n",
    "    h = int(frame.shape[0] * scale / 100)\n",
    "    return cv2.resize(frame, (w, h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def update_tracking(centers_old, center, thr, last_key, frame_idx, frame_max):\n",
    "    best_id = None\n",
    "    min_dist = float('inf')\n",
    "    for obj_id, history in centers_old.items():\n",
    "        last_center = list(history.keys())[-1]\n",
    "        dist = np.linalg.norm(np.array(last_center) - np.array(center))\n",
    "        if dist < thr and dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_id = obj_id\n",
    "\n",
    "    if best_id is not None:\n",
    "        centers_old[best_id][center] = frame_idx\n",
    "        return centers_old, best_id, False, best_id\n",
    "    else:\n",
    "        new_id = f\"p_{len(centers_old)}\"\n",
    "        centers_old[new_id] = {center: frame_idx}\n",
    "        return centers_old, new_id, True, new_id\n",
    "\n",
    "def filter_tracks(centers_old, patience):\n",
    "    current_frame = max([max(t.values()) for t in centers_old.values()] + [0])\n",
    "    to_remove = [k for k, v in centers_old.items() if current_frame - max(v.values()) > patience]\n",
    "    for k in to_remove:\n",
    "        del centers_old[k]\n",
    "    return centers_old\n",
    "\n",
    "# -------------------------------\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª\n",
    "# -------------------------------\n",
    "centers_old = {}\n",
    "total_count = 0\n",
    "frame_idx = 0\n",
    "\n",
    "print(\"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞—á–∞—Ç–∞. –ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"‚ö†Ô∏è –ö–∞–¥—Ä –Ω–µ –ø–æ–ª—É—á–µ–Ω. –ü–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ...\")\n",
    "        cap.release()\n",
    "        cap = cv2.VideoCapture(STREAM_URL)\n",
    "        continue\n",
    "\n",
    "    frame_idx += 1\n",
    "    frame = resize_frame(frame, SCALE_PERCENT)\n",
    "    display_frame = frame.copy()\n",
    "\n",
    "    # --- ROI ---\n",
    "    overlay = display_frame.copy()\n",
    "    cv2.polylines(overlay, [ROI_POINTS], True, (255, 0, 0), 2)\n",
    "    cv2.fillPoly(overlay, [ROI_POINTS], (255, 0, 0))\n",
    "    cv2.addWeighted(overlay, ALPHA, display_frame, 1 - ALPHA, 0, display_frame)\n",
    "\n",
    "    # –í—ã—Ä–µ–∑–∞–µ–º ROI\n",
    "    x_min, y_min = ROI_POINTS[:, 0].min(), ROI_POINTS[:, 1].min()\n",
    "    x_max, y_max = ROI_POINTS[:, 0].max(), ROI_POINTS[:, 1].max()\n",
    "    roi_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    # --- –î–µ—Ç–µ–∫—Ü–∏—è ---\n",
    "    results = model(roi_frame, conf=CONF_LEVEL, classes=[0], verbose=False)\n",
    "    if len(results[0].boxes) == 0:\n",
    "        detections = pd.DataFrame(columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf'])\n",
    "    else:\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        conf = results[0].boxes.conf.cpu().numpy()\n",
    "        detections = pd.DataFrame(np.column_stack([boxes, conf]),\n",
    "                                  columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf'])\n",
    "\n",
    "    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ ---\n",
    "    for _, row in detections.iterrows():\n",
    "        xmin, ymin, xmax, ymax, conf_val = map(int, row)\n",
    "        cx = (xmin + xmax) // 2 + x_min\n",
    "        cy = (ymin + ymax) // 2 + y_min\n",
    "\n",
    "        centers_old, obj_id, is_new, _ = update_tracking(\n",
    "            centers_old, (cx, cy), THR_CENTERS, None, frame_idx, FRAME_MAX\n",
    "        )\n",
    "        total_count += is_new\n",
    "\n",
    "        # –†–∏—Å—É–µ–º\n",
    "        cv2.rectangle(display_frame, (xmin + x_min, ymin + y_min), (xmax + x_min, ymax + y_min), (0, 255, 0), 2)\n",
    "        cv2.circle(display_frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "        cv2.putText(display_frame, f\"{obj_id} ({conf_val:.2f})\", (xmin + x_min, ymin + y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "\n",
    "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è\n",
    "    centers_old = filter_tracks(centers_old, PATIENCE)\n",
    "\n",
    "    # –°—á—ë—Ç—á–∏–∫\n",
    "    cv2.putText(display_frame, f\"–õ—é–¥–µ–π –≤ –∑–æ–Ω–µ: {total_count}\", (30, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2)\n",
    "\n",
    "    # –ó–∞–ø–∏—Å—å –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n",
    "    out.write(display_frame)\n",
    "    cv2.imshow(\"YOLOv8 ‚Äî –ü–∞—Å—Å–∞–∂–∏—Ä–æ–ø–æ—Ç–æ–∫\", display_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# -------------------------------\n",
    "# –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è\n",
    "# -------------------------------\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# –ü–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∫–∞\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-i\", tmp_out,\n",
    "    \"-crf\", \"18\", \"-preset\", \"veryfast\",\n",
    "    \"-vcodec\", \"libx264\", output_out\n",
    "], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "os.remove(tmp_out)\n",
    "print(f\"‚úÖ –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27cb533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Original Dim: 1920x1080, FPS: 25.0\n",
      "‚úÖ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞. –ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'obj_id' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 190\u001b[39m\n\u001b[32m    187\u001b[39m center_x += x_min\n\u001b[32m    188\u001b[39m center_y += y_min\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m centers_old, obj_id, is_new, lastKey = \u001b[43mupdate_tracking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenters_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenter_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthr_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlastKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_max\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m count_p += is_new\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# –†–∏—Å—É–µ–º bounding box –∏ —Ü–µ–Ω—Ç—Ä\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mupdate_tracking\u001b[39m\u001b[34m(centers_old, current_center, threshold, lastKey, frame_idx, frame_max)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m centers_old, best_key, \u001b[38;5;28;01mFalse\u001b[39;00m, best_key\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# –ù–æ–≤—ã–π –æ–±—ä–µ–∫—Ç\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     new_id = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mperson_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mobj_id\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m     centers_old[new_id] = {(center_x, center_y): frame_idx}\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m centers_old, new_id, \u001b[38;5;28;01mTrue\u001b[39;00m, new_id\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'obj_id' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# ### Configurations\n",
    "# -------------------------------\n",
    "verbose = False\n",
    "scale_percent = 100  # –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–¥—Ä–∞\n",
    "conf_level = 0.8     # –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "thr_centers = 20     # –ø–æ—Ä–æ–≥ –¥–ª—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–æ–≤\n",
    "frame_max = 5        # –º–∞–∫—Å. –∫–∞–¥—Ä–æ–≤ –¥–æ \"–ø–æ—Ç–µ—Ä–∏\" –æ–±—ä–µ–∫—Ç–∞\n",
    "patience = 100       # –º–∞–∫—Å. –¥–ª–∏–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏ —Ç—Ä–µ–∫–æ–≤\n",
    "alpha = 0.1          # –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å ROI\n",
    "video_name = 'result.mp4'\n",
    "\n",
    "# URL –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞\n",
    "stream_url = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "model = YOLO('yolov8n.pt')  # –∏–ª–∏ yolov10n.pt\n",
    "\n",
    "# –ö–ª–∞—Å—Å—ã: —Ç–æ–ª—å–∫–æ –ª—é–¥–∏\n",
    "class_IDS = [0]\n",
    "dict_classes = {0: 'person'}\n",
    "\n",
    "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ\n",
    "centers_old = {}\n",
    "obj_id = 0\n",
    "count_p = 0\n",
    "lastKey = ''\n",
    "\n",
    "# -------------------------------\n",
    "# –û—Ç–∫—Ä—ã–≤–∞–µ–º –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫\n",
    "# -------------------------------\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫.\")\n",
    "    exit()\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –≤–∏–¥–µ–æ\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(f'[INFO] - Original Dim: {width}x{height}, FPS: {fps}')\n",
    "\n",
    "# –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)\n",
    "if scale_percent != 100:\n",
    "    width = int(width * scale_percent / 100)\n",
    "    height = int(height * scale_percent / 100)\n",
    "    print(f'[INFO] - Dim Scaled: {width}x{height}')\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –∫–∞–¥—Ä–∞\n",
    "def risize_frame(frame, scale_percent):\n",
    "    if scale_percent == 100:\n",
    "        return frame\n",
    "    width_new = int(frame.shape[1] * scale_percent / 100)\n",
    "    height_new = int(frame.shape[0] * scale_percent / 100)\n",
    "    return cv2.resize(frame, (width_new, height_new), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# -------------------------------\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∑–∞–ø–∏—Å–∏ –≤–∏–¥–µ–æ\n",
    "# -------------------------------\n",
    "output_path = \"rep_\" + video_name\n",
    "tmp_output_path = \"tmp_\" + output_path\n",
    "\n",
    "# –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã\n",
    "for f in [tmp_output_path, output_path]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "VIDEO_CODEC = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "output_video = cv2.VideoWriter(tmp_output_path, VIDEO_CODEC, fps, (width, height))\n",
    "\n",
    "if not output_video.isOpened():\n",
    "    print(\"‚ùå –û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å VideoWriter.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# -------------------------------\n",
    "# ROI: –æ–±–ª–∞—Å—Ç—å –∏–Ω—Ç–µ—Ä–µ—Å–∞ (–æ—Å—Ç–∞–Ω–æ–≤–∫–∞)\n",
    "# -------------------------------\n",
    "def get_roi_points():\n",
    "    # –ü—Ä–∏–º–µ—Ä: —Ç—Ä–µ—É–≥–æ–ª—å–Ω–∏–∫ –∏–ª–∏ —Ç—Ä–∞–ø–µ—Ü–∏—è –Ω–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ\n",
    "    # –ê–¥–∞–ø—Ç–∏—Ä—É–π –ø–æ–¥ —Å–≤–æ—é –∫–∞–º–µ—Ä—É!\n",
    "    return [np.array([(1250, 400), (750, 400), (700, 800), (1200, 800)], np.int32)]\n",
    "\n",
    "# -------------------------------\n",
    "# –§—É–Ω–∫—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç—Ä–µ–∫–∏–Ω–≥–∞\n",
    "# -------------------------------\n",
    "def update_tracking(centers_old, current_center, threshold, lastKey, frame_idx, frame_max):\n",
    "    center_x, center_y = current_center\n",
    "    is_new = False\n",
    "    min_dist = float('inf')\n",
    "    best_key = None\n",
    "\n",
    "    for obj_id, centers in centers_old.items():\n",
    "        # –ü–æ—Å–ª–µ–¥–Ω–∏–π –∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ü–µ–Ω—Ç—Ä\n",
    "        last_center = list(centers.keys())[-1]\n",
    "        cx, cy = last_center\n",
    "        dist = np.sqrt((cx - center_x)**2 + (cy - center_y)**2)\n",
    "        if dist < min_dist and dist < threshold:\n",
    "            min_dist = dist\n",
    "            best_key = obj_id\n",
    "\n",
    "    if best_key is not None:\n",
    "        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç—Ä–µ–∫\n",
    "        centers_old[best_key][(center_x, center_y)] = frame_idx\n",
    "        return centers_old, best_key, False, best_key\n",
    "    else:\n",
    "        # –ù–æ–≤—ã–π –æ–±—ä–µ–∫—Ç\n",
    "        new_id = f\"person_{obj_id}\"\n",
    "        centers_old[new_id] = {(center_x, center_y): frame_idx}\n",
    "        return centers_old, new_id, True, new_id\n",
    "\n",
    "# -------------------------------\n",
    "# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö —Ç—Ä–µ–∫–æ–≤\n",
    "# -------------------------------\n",
    "def filter_tracks(centers_old, patience):\n",
    "    current_frame = max([max(frames.values()) for frames in centers_old.values()] + [0]) if centers_old else 0\n",
    "    keys_to_remove = []\n",
    "    for obj_id, centers in centers_old.items():\n",
    "        last_seen = max(centers.values())\n",
    "        if current_frame - last_seen > patience:\n",
    "            keys_to_remove.append(obj_id)\n",
    "    for k in keys_to_remove:\n",
    "        del centers_old[k]\n",
    "    return centers_old\n",
    "\n",
    "# -------------------------------\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–∞\n",
    "# -------------------------------\n",
    "print(\"‚úÖ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ–ø–æ—Ç–æ–∫–∞. –ù–∞–∂–º–∏—Ç–µ 'q' –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.\")\n",
    "\n",
    "frame_count = 0\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–∞–¥—Ä. –ü–æ–ø—ã—Ç–∫–∞ –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è...\")\n",
    "            time.sleep(2)\n",
    "            cap.release()\n",
    "            cap = cv2.VideoCapture(stream_url)\n",
    "            continue\n",
    "\n",
    "        frame_count += 1\n",
    "        frame = risize_frame(frame, scale_percent)\n",
    "        overlay = frame.copy()\n",
    "\n",
    "        # --- ROI ---\n",
    "        area_roi = get_roi_points()\n",
    "        cv2.polylines(overlay, pts=area_roi, isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        cv2.fillPoly(overlay, area_roi, (255, 0, 0))\n",
    "        frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "        # –í—ã—Ä–µ–∑–∞–µ–º ROI\n",
    "        x_min = min([p[0] for p in area_roi[0]])\n",
    "        x_max = max([p[0] for p in area_roi[0]])\n",
    "        y_min = min([p[1] for p in area_roi[0]])\n",
    "        y_max = max([p[1] for p in area_roi[0]])\n",
    "        roi_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        # --- –î–µ—Ç–µ–∫—Ü–∏—è ---\n",
    "        results = model.predict(roi_frame, conf=conf_level, classes=class_IDS, verbose=False)\n",
    "        if len(results[0].boxes) == 0:\n",
    "            positions_frame = pd.DataFrame(columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "        else:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "            conf = results[0].boxes.conf.cpu().numpy()\n",
    "            classes = results[0].boxes.cls.cpu().numpy()\n",
    "            positions_frame = pd.DataFrame(np.column_stack([boxes, conf, classes]),\n",
    "                                           columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "\n",
    "        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ —á–µ–ª–æ–≤–µ–∫–∞ ---\n",
    "        for idx, row in positions_frame.iterrows():\n",
    "            xmin, ymin, xmax, ymax, conf_val, cls = map(int, row[:6])\n",
    "            center_x = (xmin + xmax) // 2\n",
    "            center_y = (ymin + ymax) // 2\n",
    "\n",
    "            # –°–º–µ—â–µ–Ω–∏–µ —Ü–µ–Ω—Ç—Ä–∞ –≤ –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã\n",
    "            center_x += x_min\n",
    "            center_y += y_min\n",
    "\n",
    "            centers_old, obj_id, is_new, lastKey = update_tracking(\n",
    "                centers_old, (center_x, center_y), thr_centers, lastKey, frame_count, frame_max\n",
    "            )\n",
    "            count_p += is_new\n",
    "\n",
    "            # –†–∏—Å—É–µ–º bounding box –∏ —Ü–µ–Ω—Ç—Ä\n",
    "            cv2.rectangle(frame, (xmin + x_min, ymin + y_min), (xmax + x_min, ymax + y_min), (0, 0, 255), 2)\n",
    "            cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"{obj_id}:{conf_val:.2f}\", (xmin + x_min, ymin + y_min - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)\n",
    "\n",
    "        # --- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö —Ç—Ä–µ–∫–æ–≤ ---\n",
    "        centers_old = filter_tracks(centers_old, patience)\n",
    "\n",
    "        # --- –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å—á—ë—Ç—á–∏–∫–∞ ---\n",
    "        cv2.putText(frame, f'People in ROI: {count_p}', (30, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 2)\n",
    "\n",
    "        # --- –ó–∞–ø–∏—Å—å –∫–∞–¥—Ä–∞ ---\n",
    "        output_video.write(frame)\n",
    "\n",
    "        # --- –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç ---\n",
    "        cv2.imshow(\"Live Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF in [ord('q'), ord(' ')]:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.\")\n",
    "\n",
    "# -------------------------------\n",
    "# –§–∏–Ω–∞–ª–∏–∑–∞—Ü–∏—è: –ø–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∫–∞ –≤–∏–¥–µ–æ\n",
    "# -------------------------------\n",
    "output_video.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"üîÅ –ü–µ—Ä–µ–∫–æ–¥–∏—Ä–æ–≤–∫–∞ –≤–∏–¥–µ–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏...\")\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-i\", tmp_output_path,\n",
    "    \"-crf\", \"18\",\n",
    "    \"-preset\", \"veryfast\",\n",
    "    \"-hide_banner\", \"-loglevel\", \"error\",\n",
    "    \"-vcodec\", \"libx264\", output_path\n",
    "], check=True)\n",
    "\n",
    "os.remove(tmp_output_path)\n",
    "print(f\"‚úÖ –í–∏–¥–µ–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8495eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking samples of processed frames\n",
    "for i in [62,63, 64, 65, 66]:\n",
    "    plt.figure(figsize =( 14, 10))\n",
    "    plt.imshow(frames_list[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output video result\n",
    "frac = 0.7 \n",
    "Video(data='rep_result.mp4', embed=True, height=int(720 * frac), width=int(1280 * frac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mai (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
