{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c3bd6b",
   "metadata": {},
   "source": [
    "# Предобученная модель YOLO для детектинга людей  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2dd971",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Загрузка предобученной модели YOLOv8 (можно 'yolov8n.pt', 'yolov8s.pt' и т.д.)\n",
    "model = YOLO('yolov10n.pt')  # nano-версия (быстрая, но менее точная)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ccd8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 3 persons, 100.7ms\n",
      "Speed: 4.3ms preprocess, 100.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Количество людей на фото: 3\n"
     ]
    }
   ],
   "source": [
    "# Загрузка изображения\n",
    "#image = cv2.imread('2.png')\n",
    "image = cv2.imread('E://AIM/AI-Flow-Detecting/ai-core/data/2025-08-08_11-29-04.png')\n",
    "\n",
    "# Детекция людей (класс '0' в COCO = человек)\n",
    "results = model(image, classes=[0])  \n",
    "\n",
    "# Визуализация результатов\n",
    "annotated_image = results[0].plot()  # Рисует bounding boxes\n",
    "cv2.imshow('Detected People', annotated_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Подсчёт количества людей\n",
    "people_count = len(results[0].boxes)\n",
    "print(f\"Количество людей на фото: {people_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2747fe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Видеопоток запущен. Детекция людей через YOLOv8...\n",
      "⏹️ Работа завершена.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Загрузка модели YOLOv8 (nano — быстро, подходит для реального времени)\n",
    "model = YOLO('yolov10n.pt')\n",
    "\n",
    "# URL видеопотока (убраны лишние пробелы в конце!)\n",
    "stream_url = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"\n",
    "\n",
    "# Открываем видеопоток\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Ошибка: Не удалось открыть видеопоток. Проверь URL и соединение.\")\n",
    "    exit()\n",
    "\n",
    "print(\"✅ Видеопоток запущен. Детекция людей через YOLOv8...\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"⚠️ Не удалось получить кадр. Поток может быть разорван.\")\n",
    "        break\n",
    "\n",
    "    # Детекция ТОЛЬКО людей (класс 0 в COCO)\n",
    "    results = model.predict(frame, classes=[0], conf=0.5, verbose=False)\n",
    "\n",
    "    # Наносим bounding boxes и метки\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Подсчёт обнаруженных людей\n",
    "    people_count = len(results[0].boxes)\n",
    "    cv2.putText(annotated_frame, f'Людей: {people_count}', (10, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Показываем кадр\n",
    "    cv2.imshow('YOLOv8 — Детекция людей в реальном времени', annotated_frame)\n",
    "\n",
    "    # Выход по клавише 'q' или пробелу\n",
    "    if cv2.waitKey(1) & 0xFF in [ord('q'), ord(' ')]:\n",
    "        break\n",
    "\n",
    "# Освобождение ресурсов\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"⏹️ Работа завершена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d9ee499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Display image and videos\n",
    "import IPython\n",
    "from IPython.display import Video, display\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import urllib.request \n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a612e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сбор кадров начат... Нажми 'q' для остановки.\n",
      "Сохранён кадр 0\n",
      "Сохранён кадр 60\n",
      "Сохранён кадр 120\n",
      "Сохранён кадр 180\n",
      "Сохранён кадр 240\n",
      "Сохранён кадр 300\n",
      "Сохранён кадр 360\n",
      "Сохранён кадр 420\n",
      "Сохранён кадр 480\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data/images\", exist_ok=True)\n",
    "\n",
    "cap = cv2.VideoCapture(\"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\")\n",
    "count = 0\n",
    "\n",
    "print(\"Сбор кадров начат... Нажми 'q' для остановки.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        if count % 60 == 0:  # один кадр каждые 2 сек (30 FPS)\n",
    "            cv2.imwrite(f\"data/images/frame_{count}.jpg\", frame)\n",
    "            print(f\"Сохранён кадр {count}\")\n",
    "        count += 1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q') or count > 500:  # 500 кадров\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05094132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используется: cpu\n",
      "Ultralytics 8.3.176  Python-3.12.6 torch-2.8.0+cpu CPU (11th Gen Intel Core(TM) i7-11800H 2.30GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=0.25, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.5, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_bus_stop_finetuned, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=15, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/train, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs\\train\\yolov8n_bus_stop_finetuned, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dataset 'data.yaml' error  'data.yaml' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:607\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.data.rsplit(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myaml\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33myml\u001b[39m\u001b[33m\"\u001b[39m} \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.task \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[32m    602\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdetect\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    603\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msegment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    604\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpose\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    605\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mobb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    606\u001b[39m }:\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     data = \u001b[43mcheck_det_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    608\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m data:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\data\\utils.py:402\u001b[39m, in \u001b[36mcheck_det_dataset\u001b[39m\u001b[34m(dataset, autodownload)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03mDownload, verify, and/or unzip a dataset if not found locally.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    400\u001b[39m \u001b[33;03m    (Dict[str, Any]): Parsed dataset information and paths.\u001b[39;00m\n\u001b[32m    401\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m file = \u001b[43mcheck_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[38;5;66;03m# Download (optional)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\utils\\checks.py:568\u001b[39m, in \u001b[36mcheck_file\u001b[39m\u001b[34m(file, suffix, download, download_dir, hard)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files \u001b[38;5;129;01mand\u001b[39;00m hard:\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m does not exist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(files) > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m hard:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: 'data.yaml' does not exist",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m model = YOLO(\u001b[33m'\u001b[39m\u001b[33myolov8n.pt\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# можно yolov8s.pt для точности\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Обучение\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata.yaml\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# см. ниже\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43myolov8n_bus_stop_finetuned\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mAdamW\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43miou\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mruns/train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     27\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Сохранение\u001b[39;00m\n\u001b[32m     30\u001b[39m model.save(\u001b[33m'\u001b[39m\u001b[33mmodels/yolov8n_best.pt\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\model.py:793\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    791\u001b[39m     args[\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.ckpt_path\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer = \u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_smart_load\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrainer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_callbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.get(\u001b[33m\"\u001b[39m\u001b[33mresume\u001b[39m\u001b[33m\"\u001b[39m):  \u001b[38;5;66;03m# manually set model only if not resuming\u001b[39;00m\n\u001b[32m    795\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:153\u001b[39m, in \u001b[36mBaseTrainer.__init__\u001b[39m\u001b[34m(self, cfg, overrides, _callbacks)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m.model = check_model_file_from_stem(\u001b[38;5;28mself\u001b[39m.args.model)  \u001b[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(LOCAL_RANK):  \u001b[38;5;66;03m# avoid auto-downloading dataset multiple times\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     \u001b[38;5;28mself\u001b[39m.data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m.ema = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# Optimization utils init\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\AIM\\AI-Flow-Detecting\\ai-core\\mai\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:611\u001b[39m, in \u001b[36mBaseTrainer.get_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    609\u001b[39m             \u001b[38;5;28mself\u001b[39m.args.data = data[\u001b[33m\"\u001b[39m\u001b[33myaml_file\u001b[39m\u001b[33m\"\u001b[39m]  \u001b[38;5;66;03m# for validating 'yolo train data=url.zip' usage\u001b[39;00m\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(emojis(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_url(\u001b[38;5;28mself\u001b[39m.args.data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m error ❌ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.single_cls:\n\u001b[32m    613\u001b[39m     LOGGER.info(\u001b[33m\"\u001b[39m\u001b[33mOverriding class names with single class.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Dataset 'data.yaml' error  'data.yaml' does not exist"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "# Выбираем устройство\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Используется: {device}\")\n",
    "\n",
    "# Загружаем предобученную модель\n",
    "model = YOLO('yolov8n.pt')  # можно yolov8s.pt для точности\n",
    "\n",
    "# Обучение\n",
    "model.train(\n",
    "    data='data.yaml',        # см. ниже\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    device=device,\n",
    "    name='yolov10n_bus_stop_finetuned',\n",
    "    augment=True,\n",
    "    patience=15,\n",
    "    optimizer='AdamW',\n",
    "    lr0=1e-3,\n",
    "    iou=0.5,\n",
    "    conf=0.25,\n",
    "    project='runs/train'\n",
    ")\n",
    "\n",
    "# Сохранение\n",
    "model.save('models/yolov10n_best.pt')\n",
    "print(\"✅ Модель дообучена и сохранена.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b8d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect_on_stream.py\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# -------------------------------\n",
    "# ### Конфигурации\n",
    "# -------------------------------\n",
    "MODEL_PATH = 'runs/train/yolov8n_bus_stop_finetuned/weights/best.pt'  # твоя дообученная модель\n",
    "STREAM_URL = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"\n",
    "CONF_LEVEL = 0.6\n",
    "SCALE_PERCENT = 100\n",
    "VIDEO_NAME = \"result.mp4\"\n",
    "ROI_POINTS = np.array([(1250, 400), (750, 400), (700, 800), (1200, 800)], np.int32)\n",
    "ALPHA = 0.1\n",
    "PATIENCE = 100\n",
    "FRAME_MAX = 5\n",
    "THR_CENTERS = 25\n",
    "\n",
    "# -------------------------------\n",
    "# Загрузка модели\n",
    "# -------------------------------\n",
    "model = YOLO(MODEL_PATH)\n",
    "dict_classes = model.model.names\n",
    "\n",
    "# -------------------------------\n",
    "# Открываем поток\n",
    "# -------------------------------\n",
    "cap = cv2.VideoCapture(STREAM_URL)\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Не удалось открыть поток\")\n",
    "    exit()\n",
    "\n",
    "# Параметры видео\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "if SCALE_PERCENT != 100:\n",
    "    width = int(width * SCALE_PERCENT / 100)\n",
    "    height = int(height * SCALE_PERCENT / 100)\n",
    "\n",
    "# -------------------------------\n",
    "# VideoWriter\n",
    "# -------------------------------\n",
    "tmp_out = \"tmp_result.mp4\"\n",
    "output_out = \"rep_result.mp4\"\n",
    "\n",
    "if os.path.exists(tmp_out): os.remove(tmp_out)\n",
    "if os.path.exists(output_out): os.remove(output_out)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(tmp_out, fourcc, fps, (width, height))\n",
    "\n",
    "# -------------------------------\n",
    "# Вспомогательные функции\n",
    "# -------------------------------\n",
    "def resize_frame(frame, scale):\n",
    "    if scale == 100: return frame\n",
    "    w = int(frame.shape[1] * scale / 100)\n",
    "    h = int(frame.shape[0] * scale / 100)\n",
    "    return cv2.resize(frame, (w, h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "def update_tracking(centers_old, center, thr, last_key, frame_idx, frame_max):\n",
    "    best_id = None\n",
    "    min_dist = float('inf')\n",
    "    for obj_id, history in centers_old.items():\n",
    "        last_center = list(history.keys())[-1]\n",
    "        dist = np.linalg.norm(np.array(last_center) - np.array(center))\n",
    "        if dist < thr and dist < min_dist:\n",
    "            min_dist = dist\n",
    "            best_id = obj_id\n",
    "\n",
    "    if best_id is not None:\n",
    "        centers_old[best_id][center] = frame_idx\n",
    "        return centers_old, best_id, False, best_id\n",
    "    else:\n",
    "        new_id = f\"p_{len(centers_old)}\"\n",
    "        centers_old[new_id] = {center: frame_idx}\n",
    "        return centers_old, new_id, True, new_id\n",
    "\n",
    "def filter_tracks(centers_old, patience):\n",
    "    current_frame = max([max(t.values()) for t in centers_old.values()] + [0])\n",
    "    to_remove = [k for k, v in centers_old.items() if current_frame - max(v.values()) > patience]\n",
    "    for k in to_remove:\n",
    "        del centers_old[k]\n",
    "    return centers_old\n",
    "\n",
    "# -------------------------------\n",
    "# Основной цикл\n",
    "# -------------------------------\n",
    "centers_old = {}\n",
    "total_count = 0\n",
    "frame_idx = 0\n",
    "\n",
    "print(\"✅ Обработка начата. Нажмите 'q' для остановки.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"⚠️ Кадр не получен. Переподключение...\")\n",
    "        cap.release()\n",
    "        cap = cv2.VideoCapture(STREAM_URL)\n",
    "        continue\n",
    "\n",
    "    frame_idx += 1\n",
    "    frame = resize_frame(frame, SCALE_PERCENT)\n",
    "    display_frame = frame.copy()\n",
    "\n",
    "    # --- ROI ---\n",
    "    overlay = display_frame.copy()\n",
    "    cv2.polylines(overlay, [ROI_POINTS], True, (255, 0, 0), 2)\n",
    "    cv2.fillPoly(overlay, [ROI_POINTS], (255, 0, 0))\n",
    "    cv2.addWeighted(overlay, ALPHA, display_frame, 1 - ALPHA, 0, display_frame)\n",
    "\n",
    "    # Вырезаем ROI\n",
    "    x_min, y_min = ROI_POINTS[:, 0].min(), ROI_POINTS[:, 1].min()\n",
    "    x_max, y_max = ROI_POINTS[:, 0].max(), ROI_POINTS[:, 1].max()\n",
    "    roi_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "    # --- Детекция ---\n",
    "    results = model(roi_frame, conf=CONF_LEVEL, classes=[0], verbose=False)\n",
    "    if len(results[0].boxes) == 0:\n",
    "        detections = pd.DataFrame(columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf'])\n",
    "    else:\n",
    "        boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "        conf = results[0].boxes.conf.cpu().numpy()\n",
    "        detections = pd.DataFrame(np.column_stack([boxes, conf]),\n",
    "                                  columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf'])\n",
    "\n",
    "    # --- Обработка каждого человека ---\n",
    "    for _, row in detections.iterrows():\n",
    "        xmin, ymin, xmax, ymax, conf_val = map(int, row)\n",
    "        cx = (xmin + xmax) // 2 + x_min\n",
    "        cy = (ymin + ymax) // 2 + y_min\n",
    "\n",
    "        centers_old, obj_id, is_new, _ = update_tracking(\n",
    "            centers_old, (cx, cy), THR_CENTERS, None, frame_idx, FRAME_MAX\n",
    "        )\n",
    "        total_count += is_new\n",
    "\n",
    "        # Рисуем\n",
    "        cv2.rectangle(display_frame, (xmin + x_min, ymin + y_min), (xmax + x_min, ymax + y_min), (0, 255, 0), 2)\n",
    "        cv2.circle(display_frame, (cx, cy), 5, (0, 255, 0), -1)\n",
    "        cv2.putText(display_frame, f\"{obj_id} ({conf_val:.2f})\", (xmin + x_min, ymin + y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
    "\n",
    "    # Фильтрация\n",
    "    centers_old = filter_tracks(centers_old, PATIENCE)\n",
    "\n",
    "    # Счётчик\n",
    "    cv2.putText(display_frame, f\"Людей в зоне: {total_count}\", (30, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2)\n",
    "\n",
    "    # Запись и отображение\n",
    "    out.write(display_frame)\n",
    "    cv2.imshow(\"YOLOv8 — Пассажиропоток\", display_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# -------------------------------\n",
    "# Финализация\n",
    "# -------------------------------\n",
    "out.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Перекодировка\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-i\", tmp_out,\n",
    "    \"-crf\", \"18\", \"-preset\", \"veryfast\",\n",
    "    \"-vcodec\", \"libx264\", output_out\n",
    "], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "os.remove(tmp_out)\n",
    "print(f\"✅ Видео сохранено: {output_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27cb533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Original Dim: 1920x1080, FPS: 25.0\n",
      "✅ Начало обработки видеопотока. Нажмите 'q' для остановки.\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'obj_id' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 190\u001b[39m\n\u001b[32m    187\u001b[39m center_x += x_min\n\u001b[32m    188\u001b[39m center_y += y_min\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m centers_old, obj_id, is_new, lastKey = \u001b[43mupdate_tracking\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenters_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcenter_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthr_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlastKey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_max\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    193\u001b[39m count_p += is_new\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Рисуем bounding box и центр\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 118\u001b[39m, in \u001b[36mupdate_tracking\u001b[39m\u001b[34m(centers_old, current_center, threshold, lastKey, frame_idx, frame_max)\u001b[39m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m centers_old, best_key, \u001b[38;5;28;01mFalse\u001b[39;00m, best_key\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    117\u001b[39m     \u001b[38;5;66;03m# Новый объект\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     new_id = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mperson_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mobj_id\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    119\u001b[39m     centers_old[new_id] = {(center_x, center_y): frame_idx}\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m centers_old, new_id, \u001b[38;5;28;01mTrue\u001b[39;00m, new_id\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'obj_id' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------------\n",
    "# ### Configurations\n",
    "# -------------------------------\n",
    "verbose = False\n",
    "scale_percent = 100  # масштабирование кадра\n",
    "conf_level = 0.8     # порог уверенности\n",
    "thr_centers = 20     # порог для сопоставления центров\n",
    "frame_max = 5        # макс. кадров до \"потери\" объекта\n",
    "patience = 100       # макс. длина истории треков\n",
    "alpha = 0.1          # прозрачность ROI\n",
    "video_name = 'result.mp4'\n",
    "\n",
    "# URL видеопотока\n",
    "stream_url = \"https://restreamer.vms.evo73.ru/918335436b92ac26/stream.m3u8\"\n",
    "\n",
    "# Загрузка модели\n",
    "model = YOLO('yolov8n.pt')  # или yolov10n.pt\n",
    "\n",
    "# Классы: только люди\n",
    "class_IDS = [0]\n",
    "dict_classes = {0: 'person'}\n",
    "\n",
    "# Вспомогательные переменные\n",
    "centers_old = {}\n",
    "obj_id = 0\n",
    "count_p = 0\n",
    "lastKey = ''\n",
    "\n",
    "# -------------------------------\n",
    "# Открываем видеопоток\n",
    "# -------------------------------\n",
    "cap = cv2.VideoCapture(stream_url)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"❌ Ошибка: Не удалось открыть видеопоток.\")\n",
    "    exit()\n",
    "\n",
    "# Параметры видео\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(f'[INFO] - Original Dim: {width}x{height}, FPS: {fps}')\n",
    "\n",
    "# Масштабирование (если нужно)\n",
    "if scale_percent != 100:\n",
    "    width = int(width * scale_percent / 100)\n",
    "    height = int(height * scale_percent / 100)\n",
    "    print(f'[INFO] - Dim Scaled: {width}x{height}')\n",
    "\n",
    "# Функция для изменения размера кадра\n",
    "def risize_frame(frame, scale_percent):\n",
    "    if scale_percent == 100:\n",
    "        return frame\n",
    "    width_new = int(frame.shape[1] * scale_percent / 100)\n",
    "    height_new = int(frame.shape[0] * scale_percent / 100)\n",
    "    return cv2.resize(frame, (width_new, height_new), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# -------------------------------\n",
    "# Настройка записи видео\n",
    "# -------------------------------\n",
    "output_path = \"rep_\" + video_name\n",
    "tmp_output_path = \"tmp_\" + output_path\n",
    "\n",
    "# Удаляем старые файлы\n",
    "for f in [tmp_output_path, output_path]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "\n",
    "VIDEO_CODEC = cv2.VideoWriter_fourcc(*\"MP4V\")\n",
    "output_video = cv2.VideoWriter(tmp_output_path, VIDEO_CODEC, fps, (width, height))\n",
    "\n",
    "if not output_video.isOpened():\n",
    "    print(\"❌ Ошибка: Не удалось инициализировать VideoWriter.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# -------------------------------\n",
    "# ROI: область интереса (остановка)\n",
    "# -------------------------------\n",
    "def get_roi_points():\n",
    "    # Пример: треугольник или трапеция на остановке\n",
    "    # Адаптируй под свою камеру!\n",
    "    return [np.array([(1250, 400), (750, 400), (700, 800), (1200, 800)], np.int32)]\n",
    "\n",
    "# -------------------------------\n",
    "# Функция обновления трекинга\n",
    "# -------------------------------\n",
    "def update_tracking(centers_old, current_center, threshold, lastKey, frame_idx, frame_max):\n",
    "    center_x, center_y = current_center\n",
    "    is_new = False\n",
    "    min_dist = float('inf')\n",
    "    best_key = None\n",
    "\n",
    "    for obj_id, centers in centers_old.items():\n",
    "        # Последний известный центр\n",
    "        last_center = list(centers.keys())[-1]\n",
    "        cx, cy = last_center\n",
    "        dist = np.sqrt((cx - center_x)**2 + (cy - center_y)**2)\n",
    "        if dist < min_dist and dist < threshold:\n",
    "            min_dist = dist\n",
    "            best_key = obj_id\n",
    "\n",
    "    if best_key is not None:\n",
    "        # Обновляем трек\n",
    "        centers_old[best_key][(center_x, center_y)] = frame_idx\n",
    "        return centers_old, best_key, False, best_key\n",
    "    else:\n",
    "        # Новый объект\n",
    "        new_id = f\"person_{obj_id}\"\n",
    "        centers_old[new_id] = {(center_x, center_y): frame_idx}\n",
    "        return centers_old, new_id, True, new_id\n",
    "\n",
    "# -------------------------------\n",
    "# Фильтрация старых треков\n",
    "# -------------------------------\n",
    "def filter_tracks(centers_old, patience):\n",
    "    current_frame = max([max(frames.values()) for frames in centers_old.values()] + [0]) if centers_old else 0\n",
    "    keys_to_remove = []\n",
    "    for obj_id, centers in centers_old.items():\n",
    "        last_seen = max(centers.values())\n",
    "        if current_frame - last_seen > patience:\n",
    "            keys_to_remove.append(obj_id)\n",
    "    for k in keys_to_remove:\n",
    "        del centers_old[k]\n",
    "    return centers_old\n",
    "\n",
    "# -------------------------------\n",
    "# Основной цикл обработки потока\n",
    "# -------------------------------\n",
    "print(\"✅ Начало обработки видеопотока. Нажмите 'q' для остановки.\")\n",
    "\n",
    "frame_count = 0\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"⚠️ Не удалось получить кадр. Попытка переподключения...\")\n",
    "            time.sleep(2)\n",
    "            cap.release()\n",
    "            cap = cv2.VideoCapture(stream_url)\n",
    "            continue\n",
    "\n",
    "        frame_count += 1\n",
    "        frame = risize_frame(frame, scale_percent)\n",
    "        overlay = frame.copy()\n",
    "\n",
    "        # --- ROI ---\n",
    "        area_roi = get_roi_points()\n",
    "        cv2.polylines(overlay, pts=area_roi, isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "        cv2.fillPoly(overlay, area_roi, (255, 0, 0))\n",
    "        frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "        # Вырезаем ROI\n",
    "        x_min = min([p[0] for p in area_roi[0]])\n",
    "        x_max = max([p[0] for p in area_roi[0]])\n",
    "        y_min = min([p[1] for p in area_roi[0]])\n",
    "        y_max = max([p[1] for p in area_roi[0]])\n",
    "        roi_frame = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        # --- Детекция ---\n",
    "        results = model.predict(roi_frame, conf=conf_level, classes=class_IDS, verbose=False)\n",
    "        if len(results[0].boxes) == 0:\n",
    "            positions_frame = pd.DataFrame(columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "        else:\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy()\n",
    "            conf = results[0].boxes.conf.cpu().numpy()\n",
    "            classes = results[0].boxes.cls.cpu().numpy()\n",
    "            positions_frame = pd.DataFrame(np.column_stack([boxes, conf, classes]),\n",
    "                                           columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "\n",
    "        # --- Обработка каждого человека ---\n",
    "        for idx, row in positions_frame.iterrows():\n",
    "            xmin, ymin, xmax, ymax, conf_val, cls = map(int, row[:6])\n",
    "            center_x = (xmin + xmax) // 2\n",
    "            center_y = (ymin + ymax) // 2\n",
    "\n",
    "            # Смещение центра в глобальные координаты\n",
    "            center_x += x_min\n",
    "            center_y += y_min\n",
    "\n",
    "            centers_old, obj_id, is_new, lastKey = update_tracking(\n",
    "                centers_old, (center_x, center_y), thr_centers, lastKey, frame_count, frame_max\n",
    "            )\n",
    "            count_p += is_new\n",
    "\n",
    "            # Рисуем bounding box и центр\n",
    "            cv2.rectangle(frame, (xmin + x_min, ymin + y_min), (xmax + x_min, ymax + y_min), (0, 0, 255), 2)\n",
    "            cv2.circle(frame, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "            cv2.putText(frame, f\"{obj_id}:{conf_val:.2f}\", (xmin + x_min, ymin + y_min - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 1)\n",
    "\n",
    "        # --- Фильтрация старых треков ---\n",
    "        centers_old = filter_tracks(centers_old, patience)\n",
    "\n",
    "        # --- Отображение счётчика ---\n",
    "        cv2.putText(frame, f'People in ROI: {count_p}', (30, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255, 0, 0), 2)\n",
    "\n",
    "        # --- Запись кадра ---\n",
    "        output_video.write(frame)\n",
    "\n",
    "        # --- Показываем результат ---\n",
    "        cv2.imshow(\"Live Detection\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF in [ord('q'), ord(' ')]:\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"⏹️ Остановлено пользователем.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Финализация: перекодировка видео\n",
    "# -------------------------------\n",
    "output_video.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"🔁 Перекодировка видео для совместимости...\")\n",
    "\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "\n",
    "subprocess.run([\n",
    "    \"ffmpeg\", \"-i\", tmp_output_path,\n",
    "    \"-crf\", \"18\",\n",
    "    \"-preset\", \"veryfast\",\n",
    "    \"-hide_banner\", \"-loglevel\", \"error\",\n",
    "    \"-vcodec\", \"libx264\", output_path\n",
    "], check=True)\n",
    "\n",
    "os.remove(tmp_output_path)\n",
    "print(f\"✅ Видео сохранено: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8495eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking samples of processed frames\n",
    "for i in [62,63, 64, 65, 66]:\n",
    "    plt.figure(figsize =( 14, 10))\n",
    "    plt.imshow(frames_list[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afbe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output video result\n",
    "frac = 0.7 \n",
    "Video(data='rep_result.mp4', embed=True, height=int(720 * frac), width=int(1280 * frac))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mai (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
