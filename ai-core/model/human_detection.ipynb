{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d201d14",
   "metadata": {},
   "source": [
    "# Обнаруэение человека с уличных камер  \n",
    "\n",
    "Идентифицикация людей на близком расстоянии с помощью данных с уличных камер и OpenCV HOGDescriptor. Работа не стабильная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35f864d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e413761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(frame):\n",
    "    # initialize the HOG descriptor/person detector\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "    # using a greyscale picture, also for faster detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # detect people in the image\n",
    "    # returns the bounding boxes for the detected objects\n",
    "    boxes, weights = hog.detectMultiScale(gray, winStride=(8,8) )\n",
    "\n",
    "    boxes = np.array([[x, y, x + w, y + h] for (x, y, w, h) in boxes])\n",
    "\n",
    "    for (xA, yA, xB, yB) in boxes:\n",
    "        # display the detected boxes in the colour picture\n",
    "        cv2.rectangle(frame, (xA, yA), (xB, yB),\n",
    "                          (0, 0, 255), 2)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4329a9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"https://restreamer.vms.evo73.ru/918335436b92ac26/stream1.ts\") # IP webcam\n",
    "while(cap.isOpened()):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    processedFrame=analyze(frame)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',processedFrame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97662754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import supervision as sv\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, simpledialog, messagebox\n",
    "\n",
    "\n",
    "# Display image and videos\n",
    "# from IPython.display import Video, display\n",
    "# %matplotlib inline\n",
    "\n",
    "def resize_frame(frame, scale_percent):\n",
    "    \"\"\"Function to resize an image in a percent scale\"\"\"\n",
    "    width = int(frame.shape[1] * scale_percent / 100)\n",
    "    height = int(frame.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "\n",
    "    # resize image\n",
    "    resized = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def filter_tracks(centers, patience):\n",
    "    \"\"\"Function to filter track history\"\"\"\n",
    "    filter_dict = {}\n",
    "    for k, i in centers.items():\n",
    "        d_frames = i.items()\n",
    "        filter_dict[k] = dict(list(d_frames)[-patience:])\n",
    "    return filter_dict\n",
    "\n",
    "def update_tracking(centers_old,obj_center, thr_centers, lastKey, frame, frame_max):\n",
    "    \"\"\"Function to update track of objects\"\"\"\n",
    "    is_new = 0\n",
    "    lastpos = [(k, list(center.keys())[-1], list(center.values())[-1]) for k, center in centers_old.items()]\n",
    "    lastpos = [(i[0], i[2]) for i in lastpos if abs(i[1] - frame) <= frame_max]\n",
    "    # Calculating distance from existing centers points\n",
    "    previous_pos = [(k,obj_center) for k,centers in lastpos if (np.linalg.norm(np.array(centers) - np.array(obj_center)) < thr_centers)]\n",
    "    # if distance less than a threshold, it will update its positions\n",
    "    if previous_pos:\n",
    "        id_obj = previous_pos[0][0]\n",
    "        centers_old[id_obj][frame] = obj_center\n",
    "    # Else a new ID will be set to the given object\n",
    "    else:\n",
    "        if lastKey:\n",
    "            last = lastKey.split('D')[1]\n",
    "            id_obj = 'ID' + str(int(last)+1)\n",
    "        else:\n",
    "            id_obj = 'ID0'\n",
    "        is_new = 1\n",
    "        centers_old[id_obj] = {frame:obj_center}\n",
    "        lastKey = list(centers_old.keys())[-1]\n",
    "    return centers_old, id_obj, is_new, lastKey\n",
    "\n",
    "\n",
    "def extract_roi_from_video(video_path, regions):\n",
    "    # Callback function for mouse events\n",
    "    def mouse_callback(event, x, y, flags, param):\n",
    "        if event == cv2.EVENT_LBUTTONDOWN:\n",
    "            points.append((x, y))\n",
    "            cv2.circle(img, (x, y), 5, (0, 255, 0), -1)\n",
    "            cv2.imshow(region_name, img)\n",
    "\n",
    "    num_rois = len(regions)\n",
    "    ROIs = []\n",
    "    print(f'Extracting ROIs from {video_path} with {num_rois} regions of interest')\n",
    "    # regions = ['gazebo','mcg']\n",
    "    for i in range(num_rois):\n",
    "        region_name = regions[i]\n",
    "        # create frame generator\n",
    "        video_info = sv.VideoInfo.from_video_path(video_path)\n",
    "        generator = sv.get_video_frames_generator(video_path)\n",
    "        # acquire first video frame\n",
    "        iterator = iter(generator)\n",
    "        frame = next(iterator)\n",
    "        # sv.plot_image(frame)\n",
    "\n",
    "        # Create a window and set the callback function\n",
    "        img = frame\n",
    "        cv2.namedWindow(region_name)\n",
    "        cv2.setMouseCallback(region_name, mouse_callback)\n",
    "\n",
    "        points = []\n",
    "\n",
    "        # region_name = input(\"Enter a name for the region - \")\n",
    "\n",
    "        while True:\n",
    "            cv2.imshow(region_name, img)\n",
    "\n",
    "            # Wait for the user to press any key\n",
    "            key = cv2.waitKey(1)  # & 0xFF\n",
    "            if key == 27 or len(points) == 4:  # 'esc' key or 4 points selected\n",
    "                break\n",
    "\n",
    "        # Draw lines between the collected points\n",
    "        if len(points) == 4:\n",
    "            cv2.line(img, points[0], points[1], (0, 0, 255), 2)\n",
    "            cv2.line(img, points[1], points[2], (0, 0, 255), 2)\n",
    "            cv2.line(img, points[2], points[3], (0, 0, 255), 2)\n",
    "            cv2.line(img, points[3], points[0], (0, 0, 255), 2)\n",
    "            cv2.imshow(region_name, img)\n",
    "            cv2.waitKey(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        for i in range(2):\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "        # Return the coordinates and plot the frame with counter line\n",
    "        # sv.plot_image(img)\n",
    "        print(\"Selected Points:\", points)\n",
    "\n",
    "        # Extract the rectangular ROI based on the selected points\n",
    "        roi_x = min(points, key=lambda x: x[0])[0]\n",
    "        roi_y = min(points, key=lambda x: x[1])[1]\n",
    "        roi_width = max(points, key=lambda x: x[0])[0] - roi_x\n",
    "        roi_height = max(points, key=lambda x: x[1])[1] - roi_y\n",
    "\n",
    "        # Extract ROI from the frame\n",
    "        roi = frame[roi_y:roi_y + roi_height, roi_x:roi_x + roi_width]\n",
    "\n",
    "        x_range = [min(coord[0] for coord in points), max(coord[0] for coord in points)]\n",
    "        y_range = [min(coord[1] for coord in points), max(coord[1] for coord in points)]\n",
    "\n",
    "        # Adjust the range based on video width and height\n",
    "        x_range_final = [max(x_range[0], 0), min(x_range[1], video_info.width - 1)]\n",
    "        y_range_final = [max(y_range[0], 0), min(y_range[1], video_info.height - 1)]\n",
    "\n",
    "        rectangle_range = [x_range_final, y_range_final]\n",
    "\n",
    "        region = {\"name\": region_name,\n",
    "                  \"polygon\": points,\n",
    "                  \"range\": rectangle_range\n",
    "                  }\n",
    "        ROIs.append(region)\n",
    "\n",
    "    return ROIs\n",
    "\n",
    "\n",
    "def detect_pedestrains(video_path, target_dir, regions):\n",
    "    ### Configurations #Verbose during prediction\n",
    "    verbose = False\n",
    "    # Scaling percentage of original frame\n",
    "    scale_percent = 100\n",
    "    # model confidence level\n",
    "    conf_level = 0.25\n",
    "    # Threshold of centers ( old\\new)\n",
    "    thr_centers = 30\n",
    "    # Number of max frames to consider a object lost\n",
    "    frame_max = 10\n",
    "    # Number of max tracked centers stored\n",
    "    patience = 100\n",
    "    # ROI area color transparency\n",
    "    alpha = 0.3\n",
    "    # ------------------------------------------------------- # Reading video with cv2\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Objects to detect Yolo\n",
    "    class_IDS = [0]\n",
    "    # Auxiliary variables\n",
    "    centers_old = {}\n",
    "\n",
    "    obj_id = 0\n",
    "    end = []\n",
    "    frames_list = []\n",
    "    count_p = 0\n",
    "    lastKey = ''\n",
    "    print(f'[INFO] - Verbose during Prediction: {verbose}')\n",
    "\n",
    "    # Original information of video\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    print('[INFO] - Original Dim: ', (width, height, fps))\n",
    "\n",
    "    # Scaling Video for better performance\n",
    "    print(scale_percent)\n",
    "    if scale_percent != 100:\n",
    "        print('[INFO] - Scaling change may cause errors in pixels lines ')\n",
    "        width = int(width * scale_percent / 100)\n",
    "        height = int(height * scale_percent / 100)\n",
    "        print('[INFO] - Dim Scaled: ', (width, height))\n",
    "    print(scale_percent)\n",
    "\n",
    "    if '/' in video_path:\n",
    "        video_name = video_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    else:\n",
    "        video_name = video_path.split(\".\")[0]\n",
    "\n",
    "    result_video_name = video_name + \".mp4\"\n",
    "    result_directory = target_dir\n",
    "    # result_directory = \"/Users/kysgattu/FIS/ObjectDetection/Data/ExampleResults\"\n",
    "    # output_path = result_directory + \"/\" + result_video_name\n",
    "    annotated_video = result_directory + \"/Annotated_\" + result_video_name\n",
    "    VIDEO_CODEC = \"MP4V\"\n",
    "\n",
    "    output_video = cv2.VideoWriter(annotated_video,\n",
    "                                   cv2.VideoWriter_fourcc(*VIDEO_CODEC),\n",
    "                                   fps, (width, height))\n",
    "    model = YOLO('yolov10x.pt')\n",
    "    dict_classes = model.model.names\n",
    "    rois = extract_roi_from_video(video_path=video_path, regions=regions)\n",
    "    roi_counts = {roi['name']: 0 for roi in rois}\n",
    "    count_p_roi = 0\n",
    "    for i in tqdm(range(int(video.get(cv2.CAP_PROP_FRAME_COUNT)))):  # Outer loop iterating through each frame\n",
    "        # print(i)\n",
    "        # _, frame = video.read()\n",
    "\n",
    "        isFrame, frame = video.read()\n",
    "\n",
    "        if not isFrame:\n",
    "            break\n",
    "\n",
    "        for roi in rois:  # Inner loop iterating through each region of interest\n",
    "\n",
    "            area_roi = [np.array(roi['polygon'], dtype=np.int32)]\n",
    "\n",
    "            x_range, y_range = roi['range']\n",
    "            ROI = frame[y_range[0]:y_range[1], x_range[0]:x_range[1]]\n",
    "\n",
    "            if verbose:\n",
    "                print('Dimension Scaled(frame): ', (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "            y_hat = model.predict(ROI, conf=conf_level, classes=class_IDS, device='cpu', verbose=False)\n",
    "\n",
    "            boxes = y_hat[0].boxes.xyxy.cpu().numpy()\n",
    "            conf = y_hat[0].boxes.conf.cpu().numpy()\n",
    "            classes = y_hat[0].boxes.cls.cpu().numpy()\n",
    "\n",
    "            positions_frame = pd.DataFrame(np.concatenate([boxes, conf.reshape(-1, 1), classes.reshape(-1, 1)], axis=1),\n",
    "                                           columns=['xmin', 'ymin', 'xmax', 'ymax', 'conf', 'class'])\n",
    "\n",
    "            labels = [dict_classes[i] for i in classes]\n",
    "\n",
    "            for ix, row in enumerate(positions_frame.iterrows()):\n",
    "                xmin, ymin, xmax, ymax, confidence, category, = row[1].astype('int')\n",
    "                center_x, center_y = int(((xmax + xmin)) / 2), int((ymax + ymin) / 2)\n",
    "\n",
    "                centers_old, id_obj, is_new, lastKey = update_tracking(centers_old, (center_x, center_y), thr_centers,\n",
    "                                                                       lastKey,\n",
    "                                                                       i, frame_max)\n",
    "                roi_counts[roi['name']] += is_new\n",
    "\n",
    "                cv2.rectangle(ROI, (xmin, ymin), (xmax, ymax), (0, 0, 255), 2)\n",
    "                for center_x, center_y in centers_old[id_obj].values():\n",
    "                    cv2.circle(ROI, (center_x, center_y), 5, (0, 0, 255), -1)\n",
    "\n",
    "                cv2.putText(img=ROI, text=id_obj + ':' + str(np.round(conf[ix], 2)),\n",
    "                            org=(xmin, ymin - 10), fontFace=cv2.FONT_HERSHEY_TRIPLEX, fontScale=0.8, color=(0, 0, 255),\n",
    "                            thickness=1)\n",
    "\n",
    "            # Update count for the current ROI in the dictionary\n",
    "            # roi_counts[roi['name']] = count_p_roi\n",
    "            y_coordinate = 40\n",
    "            for region, person_count in roi_counts.items():\n",
    "                cv2.putText(img=frame, text=f'Counts People in ROI {region}:{person_count}',\n",
    "                            org=(30, y_coordinate), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            fontScale=1, color=(255, 0, 0), thickness=1)\n",
    "                y_coordinate += 50\n",
    "\n",
    "            centers_old = filter_tracks(centers_old, patience)\n",
    "            # if verbose:\n",
    "            #     print(counter_in, counter_out)\n",
    "\n",
    "            overlay = frame.copy()\n",
    "            cv2.polylines(overlay, pts=area_roi, isClosed=True, color=(255, 0, 0), thickness=2)\n",
    "            cv2.fillPoly(overlay, area_roi, (255, 0, 0))\n",
    "            frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)\n",
    "\n",
    "            frames_list.append(frame)\n",
    "            output_video.write(frame)\n",
    "\n",
    "    output_video.release()\n",
    "\n",
    "    # if os.path.exists(annotated_video):\n",
    "    #     os.remove(annotated_video)\n",
    "\n",
    "    for region, person_count in roi_counts.items():\n",
    "        print(f\"Number of persons moving {region} is {person_count}\")\n",
    "    print(f\"Annotated video saved at {annotated_video}\")\n",
    "    return roi_counts\n",
    "\n",
    "def browse_video_path():\n",
    "    file_path = filedialog.askopenfilename(title=\"Select Video File\")\n",
    "    entry_video_path.delete(0, tk.END)\n",
    "    entry_video_path.insert(0, file_path)\n",
    "\n",
    "\n",
    "def browse_target_dir():\n",
    "    dir_path = filedialog.askdirectory(title=\"Select Target Directory\")\n",
    "    entry_target_dir.delete(0, tk.END)\n",
    "    entry_target_dir.insert(0, dir_path)\n",
    "\n",
    "\n",
    "def clear_values():\n",
    "    entry_video_path.delete(0, tk.END)\n",
    "    entry_target_dir.delete(0, tk.END)\n",
    "    entry_num_regions.delete(0, tk.END)\n",
    "    entry_region_names.delete(0, tk.END)\n",
    "    result_text.config(state=tk.NORMAL)\n",
    "    result_text.delete(1.0, tk.END)  # Clear previous results\n",
    "    result_text.config(state=tk.DISABLED)\n",
    "\n",
    "\n",
    "def start_detection():\n",
    "    video_path = entry_video_path.get()\n",
    "    target_dir = entry_target_dir.get()\n",
    "    region_names = entry_region_names.get()\n",
    "\n",
    "    # Validate inputs\n",
    "    num_regions = entry_num_regions.get()\n",
    "    if not video_path or not target_dir or not region_names or not num_regions.isdigit():\n",
    "        messagebox.showerror(\"Error\", \"Please provide valid inputs.\")\n",
    "        return\n",
    "    else:\n",
    "        num_regions = int(num_regions)\n",
    "        regions.extend(region_names.split(\",\"))\n",
    "        if num_regions != len(regions):\n",
    "            messagebox.showerror(\"Error\", \"Number of regions entered does not match the specified number.\")\n",
    "            return\n",
    "        else:\n",
    "            try:\n",
    "                result_text.config(state=tk.NORMAL)\n",
    "                result_text.delete(1.0, tk.END)  # Clear previous results\n",
    "                result_text.insert(tk.END, \"Detection in progress...\\n\")\n",
    "                result_text.config(state=tk.DISABLED)\n",
    "\n",
    "                final_result = detect_pedestrains(video_path, target_dir, regions)\n",
    "\n",
    "                result_text.config(state=tk.NORMAL)\n",
    "                result_text.insert(tk.END, \"Detection completed. \\nResults:\\n\")\n",
    "                for region, count in final_result.items():\n",
    "                    result_text.insert(tk.END, f\"Number of Pedestrains moving {region}: {count}\\n\")\n",
    "                result_text.config(state=tk.DISABLED)\n",
    "\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"An error occurred during detection: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # roi_counts = detect_pedestrains(video_path='/Users/kysgattu/FIS/ObjectDetection/Data/MCG_171023_10sec.avi',\n",
    "    #                             target_dir='/Users/kysgattu/FIS/ObjectDetection/Data/ExampleResults',\n",
    "    #                             regions=[\"Towards Gazebo\",\"Along McGauvran\"])\n",
    "\n",
    "    # Create the main window\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Pedestrian Detection App\")\n",
    "\n",
    "    # Create and place widgets\n",
    "    label_video_path = tk.Label(root, text=\"Video Path:\")\n",
    "    label_video_path.grid(row=0, column=0, padx=10, pady=10, sticky=tk.E)\n",
    "\n",
    "    entry_video_path = tk.Entry(root, width=50)\n",
    "    entry_video_path.grid(row=0, column=1, padx=10, pady=10, columnspan=2)\n",
    "\n",
    "    button_browse_video = tk.Button(root, text=\"Browse\", command=browse_video_path)\n",
    "    button_browse_video.grid(row=0, column=3, padx=10, pady=10)\n",
    "\n",
    "    label_target_dir = tk.Label(root, text=\"Target Directory:\")\n",
    "    label_target_dir.grid(row=1, column=0, padx=10, pady=10, sticky=tk.E)\n",
    "\n",
    "    entry_target_dir = tk.Entry(root, width=50)\n",
    "    entry_target_dir.grid(row=1, column=1, padx=10, pady=10, columnspan=2)\n",
    "\n",
    "    button_browse_target = tk.Button(root, text=\"Browse\", command=browse_target_dir)\n",
    "    button_browse_target.grid(row=1, column=3, padx=10, pady=10)\n",
    "\n",
    "    label_num_regions = tk.Label(root, text=\"Number of Regions:\")\n",
    "    label_num_regions.grid(row=2, column=0, padx=10, pady=10, sticky=tk.E)\n",
    "\n",
    "    entry_num_regions = tk.Entry(root, width=50)\n",
    "    entry_num_regions.grid(row=2, column=1, padx=10, pady=10)\n",
    "\n",
    "    label_region_names = tk.Label(root, text=\"Region Names (comma-separated):\")\n",
    "    label_region_names.grid(row=3, column=0, padx=10, pady=10, sticky=tk.E)\n",
    "\n",
    "    entry_region_names = tk.Entry(root, width=50)\n",
    "    entry_region_names.grid(row=3, column=1, padx=10, pady=10, columnspan=2)\n",
    "\n",
    "    button_start_detection = tk.Button(root, text=\"Start Detection\", command=start_detection)\n",
    "    button_start_detection.grid(row=5, column=0, columnspan=4, pady=10)\n",
    "\n",
    "    regions = []\n",
    "\n",
    "    result_text = tk.Text(root, height=10, width=60, state=tk.DISABLED, relief=\"groove\",\n",
    "                          wrap=tk.WORD)  # ,borderwidth=3)\n",
    "    result_text.grid(row=6, column=0, columnspan=4, pady=10)\n",
    "\n",
    "    # Start the Tkinter event loop\n",
    "    root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mai (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
