{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "883db617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "def generate_density_map(image_shape, points):\n",
    "    \"\"\"\n",
    "    Генерирует density map из точек (координат голов людей).\n",
    "    \"\"\"\n",
    "    density_map = np.zeros(image_shape, dtype=np.float32)\n",
    "    h, w = image_shape\n",
    "    \n",
    "    if len(points) == 0:\n",
    "        return density_map\n",
    "    \n",
    "    # Преобразуем точки в правильный формат\n",
    "    try:\n",
    "        points = np.array(points, dtype=np.float32)\n",
    "        if points.ndim == 1:\n",
    "            points = points.reshape(-1, 2)\n",
    "    except:\n",
    "        return density_map\n",
    "    \n",
    "    # Создаём Gaussian-размытие вокруг каждой точки\n",
    "    for point in points:\n",
    "        if len(point) < 2:\n",
    "            continue\n",
    "            \n",
    "        x, y = point[0], point[1]\n",
    "        x = min(max(0, int(round(x))), w-1)\n",
    "        y = min(max(0, int(round(y))), h-1)\n",
    "        \n",
    "        pt2d = np.zeros((h, w), dtype=np.float32)\n",
    "        pt2d[y, x] = 1\n",
    "        density_map += cv2.GaussianBlur(pt2d, (15, 15), sigma=4)\n",
    "    \n",
    "    return density_map\n",
    "\n",
    "class ShanghaiTechDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.images = []\n",
    "        self.density_maps = []\n",
    "        \n",
    "        img_dir = os.path.join(root_dir, \"images\")\n",
    "        gt_dir = os.path.join(root_dir, \"ground-truth\")\n",
    "        \n",
    "        print(f\"Путь к изображениям: {img_dir}\")\n",
    "        print(f\"Путь к разметке: {gt_dir}\")\n",
    "        \n",
    "        if not os.path.exists(img_dir):\n",
    "            raise FileNotFoundError(f\"Папка {img_dir} не найдена!\")\n",
    "        if not os.path.exists(gt_dir):\n",
    "            raise FileNotFoundError(f\"Папка {gt_dir} не найдена!\")\n",
    "\n",
    "        for img_file in os.listdir(img_dir):\n",
    "            if not img_file.endswith(\".jpg\"):\n",
    "                continue\n",
    "                \n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            mat_file = f\"GT_{img_file.replace('.jpg', '.mat')}\"\n",
    "            mat_path = os.path.join(gt_dir, mat_file)\n",
    "            \n",
    "            if not os.path.exists(mat_path):\n",
    "                print(f\"Файл разметки {mat_file} не найден!\")\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Загрузка .mat файла\n",
    "                mat = scipy.io.loadmat(mat_path)\n",
    "                points = mat[\"image_info\"][0][0][0][0][0]  # Основной формат\n",
    "                \n",
    "                # Альтернативный формат, если основной пустой\n",
    "                if points.size == 0:\n",
    "                    points = mat[\"image_info\"][0][0][1][0][0]\n",
    "                \n",
    "                # Загрузка изображения\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    print(f\"Ошибка загрузки: {img_path}\")\n",
    "                    continue\n",
    "                \n",
    "                # Генерация density map\n",
    "                density_map = generate_density_map((img.shape[0], img.shape[1]), points)\n",
    "                \n",
    "                self.images.append(img)\n",
    "                self.density_maps.append(density_map)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при обработке {img_file}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        density = self.density_maps[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "        \n",
    "        return img, density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5c73e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "\n",
    "class CSRNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CSRNet, self).__init__()\n",
    "        # Frontend (VGG-16 без последнего MaxPool)\n",
    "        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "        self.frontend = nn.Sequential(*list(vgg.features.children())[:-1])  # Закрывающая скобка добавлена\n",
    "        \n",
    "        # Backend (Dilated Convolutions)\n",
    "        self.backend = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=2, dilation=2), nn.ReLU(),\n",
    "            nn.Conv2d(512, 256, 3, padding=2, dilation=2), nn.ReLU(),\n",
    "            nn.Conv2d(256, 128, 3, padding=2, dilation=2), nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, 3, padding=2, dilation=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 1, 1)  # Output: density map\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.frontend(x)\n",
    "        x = self.backend(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72e73f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShanghaiTechDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.images = []\n",
    "        self.density_maps = []\n",
    "        \n",
    "        # Исправленные пути - УБРАЛ ДУБЛИРОВАНИЕ train_data/test_data\n",
    "        img_dir = os.path.join(root_dir, \"images\")\n",
    "        gt_dir = os.path.join(root_dir, \"ground-truth\")\n",
    "        \n",
    "        print(f\"Проверка пути к изображениям: {img_dir}\")  # Для отладки\n",
    "        print(f\"Проверка пути к разметке: {gt_dir}\")      # Для отладки\n",
    "        \n",
    "        # Проверка существования путей\n",
    "        if not os.path.exists(img_dir):\n",
    "            raise FileNotFoundError(f\"Папка с изображениями не найдена: {img_dir}\")\n",
    "        if not os.path.exists(gt_dir):\n",
    "            raise FileNotFoundError(f\"Папка с разметкой не найдена: {gt_dir}\")\n",
    "\n",
    "        for img_file in os.listdir(img_dir):\n",
    "            if img_file.endswith(\".jpg\"):\n",
    "                img_path = os.path.join(img_dir, img_file)\n",
    "                mat_file = f\"GT_{img_file.replace('.jpg', '.mat')}\"\n",
    "                mat_path = os.path.join(gt_dir, mat_file)\n",
    "                \n",
    "                if not os.path.exists(mat_path):\n",
    "                    print(f\"Предупреждение: файл разметки {mat_file} не найден\")\n",
    "                    continue\n",
    "                \n",
    "                # Загрузка .mat-файла\n",
    "                mat = scipy.io.loadmat(mat_path)\n",
    "                points = mat[\"image_info\"][0][0][0][0][0]  # Координаты людей\n",
    "                \n",
    "                # Генерация density map\n",
    "                img = cv2.imread(img_path)\n",
    "                if img is None:\n",
    "                    print(f\"Предупреждение: не удалось загрузить изображение {img_path}\")\n",
    "                    continue\n",
    "                    \n",
    "                density_map = generate_density_map((img.shape[0], img.shape[1]), points)\n",
    "                \n",
    "                self.images.append(img)\n",
    "                self.density_maps.append(density_map)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        density = self.density_maps[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Нормализация\n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "        \n",
    "        return img, density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "148bfd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_density_map(image_shape, points):\n",
    "    \"\"\"\n",
    "    Генерирует density map из точек (координат голов людей).\n",
    "    Теперь с правильным вызовом GaussianBlur.\n",
    "    \"\"\"\n",
    "    density_map = np.zeros(image_shape, dtype=np.float32)\n",
    "    h, w = image_shape\n",
    "    \n",
    "    if len(points) == 0:\n",
    "        return density_map\n",
    "    \n",
    "    # Преобразуем точки в правильный формат\n",
    "    try:\n",
    "        points = np.array(points, dtype=np.float32)\n",
    "        if points.ndim == 1:\n",
    "            points = points.reshape(-1, 2)\n",
    "    except:\n",
    "        return density_map\n",
    "    \n",
    "    # Создаём Gaussian-размытие вокруг каждой точки\n",
    "    for point in points:\n",
    "        if len(point) < 2:\n",
    "            continue\n",
    "            \n",
    "        x, y = point[0], point[1]\n",
    "        x = min(max(0, int(round(x))), w-1)\n",
    "        y = min(max(0, int(round(y))), h-1)\n",
    "        \n",
    "        pt2d = np.zeros((h, w), dtype=np.float32)\n",
    "        pt2d[y, x] = 1\n",
    "        \n",
    "        # Исправленный вызов GaussianBlur:\n",
    "        # Теперь явно указываем sigmaX (sigmaY будет равен sigmaX, если не указан)\n",
    "        density_map += cv2.GaussianBlur(pt2d, (15, 15), sigmaX=4)\n",
    "    \n",
    "    return density_map\n",
    "\n",
    "\n",
    "class ShanghaiTechDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, train=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.images = []\n",
    "        self.density_maps = []\n",
    "        \n",
    "        img_dir = os.path.join(root_dir, \"images\")\n",
    "        gt_dir = os.path.join(root_dir, \"ground-truth\")\n",
    "        \n",
    "        print(f\"Проверка пути к изображениям: {img_dir}\")\n",
    "        print(f\"Проверка пути к разметке: {gt_dir}\")\n",
    "        \n",
    "        if not os.path.exists(img_dir):\n",
    "            raise FileNotFoundError(f\"Папка с изображениями не найдена: {img_dir}\")\n",
    "        if not os.path.exists(gt_dir):\n",
    "            raise FileNotFoundError(f\"Папка с разметкой не найдена: {gt_dir}\")\n",
    "\n",
    "        for img_file in os.listdir(img_dir):\n",
    "            if img_file.endswith(\".jpg\"):\n",
    "                img_path = os.path.join(img_dir, img_file)\n",
    "                mat_file = f\"GT_{img_file.replace('.jpg', '.mat')}\"\n",
    "                mat_path = os.path.join(gt_dir, mat_file)\n",
    "                \n",
    "                if not os.path.exists(mat_path):\n",
    "                    print(f\"Предупреждение: файл разметки {mat_file} не найден\")\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    # Загрузка .mat-файла\n",
    "                    mat = scipy.io.loadmat(mat_path)\n",
    "                    points = mat[\"image_info\"][0][0][0][0][0]  # Координаты людей\n",
    "                    \n",
    "                    # Альтернативный вариант чтения точек\n",
    "                    if points.size == 0:\n",
    "                        points = mat[\"image_info\"][0][0][1][0][0]\n",
    "                    \n",
    "                    # Загрузка изображения\n",
    "                    img = cv2.imread(img_path)\n",
    "                    if img is None:\n",
    "                        print(f\"Предупреждение: не удалось загрузить изображение {img_path}\")\n",
    "                        continue\n",
    "                        \n",
    "                    # Генерация density map с исправленной функцией\n",
    "                    density_map = generate_density_map((img.shape[0], img.shape[1]), points)\n",
    "                    \n",
    "                    self.images.append(img)\n",
    "                    self.density_maps.append(density_map)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Ошибка при обработке {img_file}: {str(e)}\")\n",
    "                    continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.images[idx]\n",
    "        density = self.density_maps[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        img = transforms.functional.to_tensor(img)\n",
    "        density = torch.from_numpy(density).unsqueeze(0).float()\n",
    "        \n",
    "        return img, density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c336fbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка пути к изображениям: data\\part_A\\train_data\\images\n",
      "Проверка пути к разметке: data\\part_A\\train_data\\ground-truth\n",
      "Проверка пути к изображениям: data\\part_A\\test_data\\images\n",
      "Проверка пути к разметке: data\\part_A\\test_data\\ground-truth\n"
     ]
    }
   ],
   "source": [
    "# Пути к данным\n",
    "train_data_path = os.path.join(\"data\", \"part_A\", \"train_data\")\n",
    "test_data_path = os.path.join(\"data\", \"part_A\", \"test_data\")\n",
    "\n",
    "# Трансформации для изображений\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),  # Приводим все изображения к одному размеру\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Нормализация для VGG\n",
    "])\n",
    "\n",
    "# Создаем датасеты\n",
    "train_dataset = ShanghaiTechDataset(train_data_path, transform, train=True)\n",
    "test_dataset = ShanghaiTechDataset(test_data_path, transform, train=False)\n",
    "\n",
    "# Создаем DataLoader'ы\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06cf8449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to C:\\Users\\Egor/.cache\\torch\\hub\\checkpoints\\vgg16-397923af.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1.9%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(torch.cuda.is_available())\n\u001b[32m      3\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m model = \u001b[43mCSRNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Функция потерь и оптимизатор\u001b[39;00m\n\u001b[32m      7\u001b[39m criterion = nn.MSELoss()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mCSRNet.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28msuper\u001b[39m(CSRNet, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Frontend (VGG-16 без последнего MaxPool)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m vgg = \u001b[43mvgg16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mVGG16_Weights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mIMAGENET1K_V1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mself\u001b[39m.frontend = nn.Sequential(*\u001b[38;5;28mlist\u001b[39m(vgg.features.children())[:-\u001b[32m1\u001b[39m])  \u001b[38;5;66;03m# Закрывающая скобка добавлена\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Backend (Dilated Convolutions)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\learn\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[39m, in \u001b[36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    135\u001b[39m     warnings.warn(\n\u001b[32m    136\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs.keys()),\u001b[38;5;250m \u001b[39mseparate_last=\u001b[33m'\u001b[39m\u001b[33mand \u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as positional \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m     kwargs.update(keyword_only_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\learn\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[39m, in \u001b[36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[32m    226\u001b[39m     kwargs[weights_param] = default_weights_arg\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\learn\\.venv\\Lib\\site-packages\\torchvision\\models\\vgg.py:433\u001b[39m, in \u001b[36mvgg16\u001b[39m\u001b[34m(weights, progress, **kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"VGG-16 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[32m    414\u001b[39m \n\u001b[32m    415\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    429\u001b[39m \u001b[33;03m    :members:\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    431\u001b[39m weights = VGG16_Weights.verify(weights)\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vgg\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\learn\\.venv\\Lib\\site-packages\\torchvision\\models\\vgg.py:105\u001b[39m, in \u001b[36m_vgg\u001b[39m\u001b[34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     model.load_state_dict(\u001b[43mweights\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\learn\\.venv\\Lib\\site-packages\\torchvision\\models\\_api.py:91\u001b[39m, in \u001b[36mWeightsEnum.get_state_dict\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\learn\\.venv\\Lib\\site-packages\\torch\\hub.py:871\u001b[39m, in \u001b[36mload_state_dict_from_url\u001b[39m\u001b[34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[39m\n\u001b[32m    869\u001b[39m         r = HASH_REGEX.search(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[32m    870\u001b[39m         hash_prefix = r.group(\u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m871\u001b[39m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[32m    874\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Egor\\Desktop\\ss\\learn\\.venv\\Lib\\site-packages\\torch\\hub.py:748\u001b[39m, in \u001b[36mdownload_url_to_file\u001b[39m\u001b[34m(url, dst, hash_prefix, progress)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(\n\u001b[32m    741\u001b[39m     total=file_size,\n\u001b[32m    742\u001b[39m     disable=\u001b[38;5;129;01mnot\u001b[39;00m progress,\n\u001b[32m   (...)\u001b[39m\u001b[32m    745\u001b[39m     unit_divisor=\u001b[32m1024\u001b[39m,\n\u001b[32m    746\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m    747\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m         buffer = \u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREAD_DATA_CHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    749\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) == \u001b[32m0\u001b[39m:\n\u001b[32m    750\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:720\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    721\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    722\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Инициализация модели\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CSRNet().to(device)\n",
    "\n",
    "# Функция потерь и оптимизатор\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324eb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (images, targets) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass и оптимизация\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Выводим статистику\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bde360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    total_mae = 0.0\n",
    "    total_mse = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Подсчет людей\n",
    "            pred_count = torch.sum(outputs).item()\n",
    "            true_count = torch.sum(targets).item()\n",
    "            \n",
    "            # Вычисление метрик\n",
    "            total_mae += abs(pred_count - true_count)\n",
    "            total_mse += (pred_count - true_count)**2\n",
    "    \n",
    "    mae = total_mae / len(test_loader)\n",
    "    mse = (total_mse / len(test_loader))**0.5\n",
    "    \n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    return mae, mse\n",
    "\n",
    "# Запуск оценки\n",
    "mae, mse = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe57201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_results(model, test_loader, num_samples=3):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(test_loader):\n",
    "            if i >= num_samples:\n",
    "                break\n",
    "                \n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Денормализация изображения\n",
    "            img = images[0].cpu().permute(1, 2, 0).numpy()\n",
    "            img = (img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # Получаем карты плотности\n",
    "            pred_density = outputs[0][0].cpu().numpy()\n",
    "            true_density = targets[0][0].cpu().numpy()\n",
    "            \n",
    "            # Подсчет людей\n",
    "            pred_count = np.sum(pred_density)\n",
    "            true_count = np.sum(true_density)\n",
    "            \n",
    "            # Визуализация\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(\"Original Image\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow(true_density, cmap='jet')\n",
    "            plt.title(f\"True Density (Count: {true_count:.1f})\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow(pred_density, cmap='jet')\n",
    "            plt.title(f\"Predicted Density (Count: {pred_count:.1f})\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.show()\n",
    "\n",
    "# Запуск визуализации\n",
    "visualize_results(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"csrnet_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9be390",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = CSRNet().to(device)\n",
    "loaded_model.load_state_dict(torch.load(\"csrnet_model.pth\"))\n",
    "loaded_model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
